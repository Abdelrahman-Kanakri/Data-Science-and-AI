{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e38964f-6660-4932-9dc2-24ca69563a7b",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do some **NLP** pipeline steps on some *Neural Network* lectures\n",
    "\n",
    "these steps contains the follwoing:\n",
    "<ol>\n",
    "    <li>Data Gathering</li>\n",
    "    <li>Data Extraction & Cleanups</li>\n",
    "    <li>Data preprocessing</li>\n",
    "    <li>Feature engineering</li>\n",
    "    <li>Model Building, Monitoring and Deployment</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "this notebook is designed by codebasics youtube channel:\n",
    "__[Codebasics NLP playlist](https://www.youtube.com/watch?v=R-AG4-qZs1A&list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX&pp=iAQB)__\n",
    "\n",
    "Data source: __[Neural Network Lectures](https://drive.google.com/drive/folders/1tzDpYU9zgY0NZ9PbVC0EiDsSAIWNznBU?usp=drive_link)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b0648-8f34-402f-970a-7644684c54ac",
   "metadata": {},
   "source": [
    "### Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29eb5aa5-467d-4122-974a-227d25b87906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a1f39-bda5-4273-a859-541177bf81a7",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "In our case, I want to scrap one of the neural networks lectures that I have studeid in my college called \"Lec 8.pdf\"\n",
    "and I have used *PyPDF2* package to extract just 2 pages of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc563ce1-a3f5-440b-89bc-b86003e4d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lec 8.pdf\", \"rb\")\n",
    "pdfReader = PyPDF2.PdfReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb5cc37-c199-43b0-a74a-27e454574b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to know the number of pages\n",
    "len(pdfReader.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506e4b0-59a9-44af-a750-00b9b57f1b9b",
   "metadata": {},
   "source": [
    "### Data Exraction & Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928de17-fc8b-4d3f-a2df-36fcf5b48354",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102163bd-30fa-482a-be2c-ba97e2a6aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "page = 1\n",
    "page2 = pdfReader.pages[page]\n",
    "page3 = pdfReader.pages[page+1]\n",
    "text_list.append(page2.extract_text())\n",
    "text_list.append(page3.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac476b5b-dd70-47e5-849e-c7ea49baf00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['•In this lecture, we take a completely different approach. Specifically, we \\nsolve the problem of classifying nonlinearly separable patterns by \\nproceeding in a hybrid manner, involving two stages: \\n•The first stage transforms a given set of nonlinearly separable patterns \\ninto a new set for which, under certain conditions, the likelihood of the \\ntransformed patterns becoming linearly separable is high; the \\nmathematical justification of this transformation is traced to an early \\npaper by Cover (1965 ).\\n•The second stage completes the solution to the prescribed classification \\nproblem by using least -squares estimation ',\n",
       " '•using a radial -basis function (RBF) network , the structure of which \\nconsists of only three layers: \\n•The input layer is made up of source nodes (sensory units) that connect the network to its \\nenvironment.\\n•The second layer, consisting of hidden units , applies a nonlinear transformation\\nfrom the input space to the hidden (feature) space. For most applications, the\\ndimensionality of the only hidden layer of the network is high; this layer is trained\\nin an unsupervised manner using stage 1 of the hybrid learning procedure .\\n•The output layer is linear , designed to supply the response of the network to the\\nactivation pattern applied to the input layer; this layer is trained in a supervised\\nmanner using stage 2 of the hybrid procedure.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d6f69f-29ff-4a9a-b5be-e6867816a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66265d74-49c5-4027-8563-4ead610d8b9f",
   "metadata": {},
   "source": [
    "**For extracting all Pages**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88af8040-0c21-4559-942f-8df7d3a4131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allPages = [0] \n",
    "for page in range(len(pdfReader.pages)):\n",
    "    page = pdfReader.pages[page]\n",
    "    allPages.append(page.extract_text())\n",
    "#uncomment the below line and run it for print all lecture\n",
    "#lecture = ' '.join(allPages) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82366f66-ab94-4e10-b2f3-ab16227cce24",
   "metadata": {},
   "source": [
    "#### Data Cleanup\n",
    "We're going to data cleanup, such as removing punctuation marks using Regex **\"re\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f783bc-bf39-44ba-b564-4c4495a6e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e16da6-986f-4022-bae9-69676ee45167",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = re.sub('[^A-Za-z0-9]+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a24138d2-beec-4703-a07b-654d867f8012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this lecture we take a completely different approach Specifically we solve the problem of classifying nonlinearly separable patterns by proceeding in a hybrid manner involving two stages The first stage transforms a given set of nonlinearly separable patterns into a new set for which under certain conditions the likelihood of the transformed patterns becoming linearly separable is high the mathematical justification of this transformation is traced to an early paper by Cover 1965 The second stage completes the solution to the prescribed classification problem by using least squares estimation using a radial basis function RBF network the structure of which consists of only three layers The input layer is made up of source nodes sensory units that connect the network to its environment The second layer consisting of hidden units applies a nonlinear transformation from the input space to the hidden feature space For most applications the dimensionality of the only hidden layer of the network is high this layer is trained in an unsupervised manner using stage 1 of the hybrid learning procedure The output layer is linear designed to supply the response of the network to the activation pattern applied to the input layer this layer is trained in a supervised manner using stage 2 of the hybrid procedure '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc9dac-cce7-4fef-9114-b4124a12bbdb",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We have now 4 steps of text preprocessing:\n",
    "<ol>\n",
    "    <li>Sentence Tokenization</li>\n",
    "    <li>Word Tokenization</li>\n",
    "    <li>POS tagging, Stemming & Lemmatization</li>\n",
    "    <li>NER: Named Entity Recognition</li>\n",
    "    <li>Stop Words Removal</li>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "for setence tokenization, there is no mean to do it after we have removed all the punctuations marks, so we're heading to word tokenization.\n",
    "\n",
    "\n",
    "In the process of tokenization, there are two libraries are specified with tokenization and they <code>Spacy</code> and <code>NLTK</code>, and am going to use them both.\n",
    "\n",
    "if you want to know the defference between the both of them, you can click the following link: \n",
    "\n",
    "__[Spacy vs NLTK](https://www.youtube.com/watch?v=h2kBNEShsiE&t=796s)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb8714-0321-4245-a486-d6475e7f565a",
   "metadata": {},
   "source": [
    "#### 2) Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc195d-e875-4443-ba55-dd4ce30f5255",
   "metadata": {},
   "source": [
    "##### Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c6bceda-3aa3-40d4-8a9c-0f2664f43b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(data = {}, columns=[\"NLTK words\", \"Spacy Words\"])\n",
    "# this data frame to hold the words in a fancy look for tokenized words with NLTK & Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40af3f5b-b31c-4e57-b206-6d578d456f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "# just in mind, to do sentence tokenizer, uncomment the following\n",
    "#from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af70c9ae-ebe0-4c9b-8121-6e0b00b2e086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk_word_tokens = word_tokenize(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8ff2e27-d697-4ee6-b786-22b113b3bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"NLTK words\"] = nltk_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bc14c-490f-4d45-8bca-e70c84ad1a17",
   "metadata": {},
   "source": [
    "##### Tokenization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecbf5eac-4c65-41ce-9db6-2ed601b7dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # you can do large by writting \"lm\" for largers package\n",
    "\n",
    "text_spacy = nlp(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "282185b1-980f-4e48-bc09-77188e68e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_word_tokens = []\n",
    "for token in text_spacy:\n",
    "    if not token.is_space: # there also token.is_punc  we didnt use it because already removed any punctuations\n",
    "        spacy_word_tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6f3f30-1ca4-412c-9852-68e3629dacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Spacy Words\"] = spacy_word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8ff8085-20b3-471d-ad55-d32f41e43b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words\n",
       "0           In          In\n",
       "1         this        this\n",
       "2      lecture     lecture\n",
       "3           we          we\n",
       "4         take        take\n",
       "..         ...         ...\n",
       "207          2           2\n",
       "208         of          of\n",
       "209        the         the\n",
       "210     hybrid      hybrid\n",
       "211  procedure   procedure\n",
       "\n",
       "[212 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214e38b-af47-428d-9ead-b53502573cf2",
   "metadata": {},
   "source": [
    "#### 3) POS Tagging, Stemming & Lemmatization\n",
    "here we're gonig to do the same as tokenization with **Spacy** & **NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dcfeca-0903-4563-b858-ac2a8e5842b8",
   "metadata": {},
   "source": [
    "##### Stemming\n",
    "stemming means removing any words addition such as suffiex, prefix, etc. using herustic rules\n",
    "\n",
    "ex:\n",
    " 1) eating--> eat\n",
    " 2) talking--> talk\n",
    " 3) adjustable-->adjust\n",
    "\n",
    "sometimes using stemming us kinda stupid, since its a fixed rule, there will be some issue.\n",
    "\n",
    "for example: \n",
    " 1) ability-->abil (romved *ity*)\n",
    "\n",
    "\n",
    "So, in Spacy, there is no stemming because they (spacy authors) choosed lemmatization since its more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918d74e-6c62-4aa2-a93c-1cb7ffe20a6c",
   "metadata": {},
   "source": [
    "###### **Stemming using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f00860c3-9d88-417f-8f2f-49d9866523dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmerNLTk = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b4a31ae-7897-4389-a8a9-678e706bb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Stemmed word with NLTK\"] = {} # adding a new column for stemmed words for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d93aa5c7-042c-4be4-9803-330281838e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stemmed_words = [] # a list for hodling the stemmed words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4118675-a18b-4586-abb5-5fdf6a060ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words[\"NLTK words\"]:\n",
    "    nltk_stemmed_words.append(stemmerNLTk.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aeb4a1a-fb25-455c-9082-a1bb1f442cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK\n",
       "0           In          In                     in\n",
       "1         this        this                    thi\n",
       "2      lecture     lecture                 lectur\n",
       "3           we          we                     we\n",
       "4         take        take                   take\n",
       "..         ...         ...                    ...\n",
       "207          2           2                      2\n",
       "208         of          of                     of\n",
       "209        the         the                    the\n",
       "210     hybrid      hybrid                 hybrid\n",
       "211  procedure   procedure               procedur\n",
       "\n",
       "[212 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[\"Stemmed word with NLTK\"] = nltk_stemmed_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b7351-62b1-4f02-ac86-2721e0ad0487",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    "lemmatization means is getting the base word (also its a lemma) using the knowledge of the language.\n",
    "\n",
    "ex:\n",
    "   1) ate-->eat\n",
    "   2) talked--> talk\n",
    "   3) written-->write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf08cc5-fddd-40f0-aeb4-9b51ea2c988c",
   "metadata": {},
   "source": [
    "###### **Lemmatization using Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787397c6-4145-4188-8834-322be6ccb925",
   "metadata": {},
   "source": [
    "Try to make it for whole lectures of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "209a0fad-c1da-4d29-8f6e-47ea8e0d2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Lemmatized words with SPacy\"] = {}\n",
    "spacy_lemmatized_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28704dd4-4c4c-4c98-95bb-2af0961f61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text_spacy:\n",
    "    if not token.is_space: \n",
    "        spacy_lemmatized_words.append(token.lemma_)\n",
    "\n",
    "words[\"Lemmatized words with SPacy\"] = spacy_lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ea6c683-42f8-43e7-a3c4-73ba6f6f2582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "      <th>Lemmatized words with SPacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "      <td>lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "      <td>procedure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK Lemmatized words with SPacy\n",
       "0           In          In                     in                          in\n",
       "1         this        this                    thi                        this\n",
       "2      lecture     lecture                 lectur                     lecture\n",
       "3           we          we                     we                          we\n",
       "4         take        take                   take                        take\n",
       "..         ...         ...                    ...                         ...\n",
       "207          2           2                      2                           2\n",
       "208         of          of                     of                          of\n",
       "209        the         the                    the                         the\n",
       "210     hybrid      hybrid                 hybrid                      hybrid\n",
       "211  procedure   procedure               procedur                   procedure\n",
       "\n",
       "[212 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b31314-9710-4343-b5ea-58f3c2fd6646",
   "metadata": {},
   "source": [
    "in case we want to do a customization behaviour of the model.\n",
    "it goes as the following\n",
    "\n",
    "lets we want to get attribute ruler function and add some customization to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "669053a5-a98b-41b6-ba5d-0b530e09fcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44168db1-a83b-4388-8127-b8ba6edd7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "ar.add([[{\"TEXT\": \"Bro\"}], [{\"TEXT\": \"Brah\"}]], {\"LEMMA\": \"Brother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0b39c67-ed79-4db0-a115-1a3e84a8acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bf8e250-d555-4907-b7be-8a7e4f0de2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro  |  Brother\n",
      ",  |  ,\n",
      "you  |  you\n",
      "wanna  |  wanna\n",
      "go  |  go\n",
      "?  |  ?\n",
      "Brah  |  Brother\n",
      ",  |  ,\n",
      "do  |  do\n",
      "n't  |  not\n",
      "say  |  say\n",
      "no  |  no\n",
      "!  |  !\n",
      "I  |  I\n",
      "am  |  be\n",
      "exhausted  |  exhaust\n"
     ]
    }
   ],
   "source": [
    "for token in sent:\n",
    "    print(token,\" | \",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5b09e-66d2-4238-b840-8a3682f7c065",
   "metadata": {},
   "source": [
    "<code>Here we made a customization behaviour on the pipeline as above</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb37022-0925-4882-b737-9acbc6ffde76",
   "metadata": {},
   "source": [
    "###### **Lemmatization using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcb917a9-4da8-4c7c-83c5-8f86837d8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmaNLTK = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9a7d3be-ad2b-4537-a904-d3026053be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Lemmatized words with NLTK\"] = {}\n",
    "nltk_lemmatized_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a77e689-1556-4bad-946b-b641c3e0e74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "      <th>Lemmatized words with SPacy</th>\n",
       "      <th>Lemmatized words with NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK Lemmatized words with SPacy  \\\n",
       "0           In          In                     in                          in   \n",
       "1         this        this                    thi                        this   \n",
       "2      lecture     lecture                 lectur                     lecture   \n",
       "3           we          we                     we                          we   \n",
       "4         take        take                   take                        take   \n",
       "..         ...         ...                    ...                         ...   \n",
       "207          2           2                      2                           2   \n",
       "208         of          of                     of                          of   \n",
       "209        the         the                    the                         the   \n",
       "210     hybrid      hybrid                 hybrid                      hybrid   \n",
       "211  procedure   procedure               procedur                   procedure   \n",
       "\n",
       "    Lemmatized words with NLTK  \n",
       "0                           In  \n",
       "1                         this  \n",
       "2                      lecture  \n",
       "3                           we  \n",
       "4                         take  \n",
       "..                         ...  \n",
       "207                          2  \n",
       "208                         of  \n",
       "209                        the  \n",
       "210                     hybrid  \n",
       "211                  procedure  \n",
       "\n",
       "[212 rows x 5 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in words[\"NLTK words\"]:\n",
    "    nltk_lemmatized_words.append(lemmaNLTK.lemmatize(word))\n",
    "words[\"Lemmatized words with NLTK\"] = nltk_lemmatized_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778c344-e58d-4ae5-82b5-0660dcd6ef66",
   "metadata": {},
   "source": [
    "Stemming:\n",
    "\n",
    "Stemming in general focuses on chopping off prefixes and suffixes to reduce a word to its base or root form. Popular stemmers, such as the Porter or Snowball stemmer, tend to lower the case of the input word by default (even if it's in uppercase).\n",
    "For example, stemming both \"Running\" and \"running\" would typically result in \"run\" (in lowercase).\n",
    "\n",
    "Lemmatization (NLTK):\n",
    "\n",
    "Lemmatization, especially when using NLTK's WordNetLemmatizer, generally does not change the case of the input word. It retains the case (lowercase or uppercase) of the original word. So if you lemmatize \"Running\" or \"running\", it will map both to \"Running\" and \"running\", respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18750551-66b9-4337-af5e-61895df95132",
   "metadata": {},
   "source": [
    "##### POS\n",
    "\n",
    "\n",
    "Here where you can find everything for **POS tagging** documentation\n",
    "\n",
    "__[Spacy Doc](https://v2.spacy.io/api/annotation#pos-universal)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf743be-9a1e-49a8-ac37-4be5e23475ba",
   "metadata": {},
   "source": [
    "###### **POS using Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6a63e95-035e-421a-bf49-8c2d3b7153f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Tags with Spacy\"] = {}\n",
    "words[\"ID tags with Spacy\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80aea342-dc1d-4adb-b1a1-ca19b4e1ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_spacy = []\n",
    "tags_id_spacy = []\n",
    "for token in text_spacy:\n",
    "    if not token.is_space:\n",
    "        tags_spacy.append(token.pos_)\n",
    "        tags_id_spacy.append(token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4370dfcb-54f0-4d8f-8585-c32a216c3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Tags with Spacy\"] = tags_spacy\n",
    "words[\"ID tags with Spacy\"] = tags_id_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd128076-cf2a-415d-9fe6-55821b6d06e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "      <th>Lemmatized words with SPacy</th>\n",
       "      <th>Lemmatized words with NLTK</th>\n",
       "      <th>Tags with Spacy</th>\n",
       "      <th>ID tags with Spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>In</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK Lemmatized words with SPacy  \\\n",
       "0           In          In                     in                          in   \n",
       "1         this        this                    thi                        this   \n",
       "2      lecture     lecture                 lectur                     lecture   \n",
       "3           we          we                     we                          we   \n",
       "4         take        take                   take                        take   \n",
       "..         ...         ...                    ...                         ...   \n",
       "207          2           2                      2                           2   \n",
       "208         of          of                     of                          of   \n",
       "209        the         the                    the                         the   \n",
       "210     hybrid      hybrid                 hybrid                      hybrid   \n",
       "211  procedure   procedure               procedur                   procedure   \n",
       "\n",
       "    Lemmatized words with NLTK Tags with Spacy  ID tags with Spacy  \n",
       "0                           In             ADP                  85  \n",
       "1                         this             DET                  90  \n",
       "2                      lecture            NOUN                  92  \n",
       "3                           we            PRON                  95  \n",
       "4                         take            VERB                 100  \n",
       "..                         ...             ...                 ...  \n",
       "207                          2             NUM                  93  \n",
       "208                         of             ADP                  85  \n",
       "209                        the             DET                  90  \n",
       "210                     hybrid             ADJ                  84  \n",
       "211                  procedure            NOUN                  92  \n",
       "\n",
       "[212 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e72657f-abdf-4576-8d93-d0a3f33124ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice  |  84  |  ADJ  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "borther  |  92  |  NOUN  |  NN  |  noun, singular or mass\n",
      "!  |  97  |  PUNCT  |  .  |  punctuation mark, sentence closer\n",
      ",  |  97  |  PUNCT  |  ,  |  punctuation mark, comma\n",
      "and  |  89  |  CCONJ  |  CC  |  conjunction, coordinating\n",
      "wow  |  91  |  INTJ  |  UH  |  interjection\n",
      "that  |  95  |  PRON  |  DT  |  determiner\n",
      "was  |  87  |  AUX  |  VBD  |  verb, past tense\n",
      "so  |  86  |  ADV  |  RB  |  adverb\n",
      "profesional  |  84  |  ADJ  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
      "code  |  92  |  NOUN  |  NN  |  noun, singular or mass\n",
      "write  |  100  |  VERB  |  VB  |  verb, base form\n",
      "there  |  86  |  ADV  |  RB  |  adverb\n",
      "!  |  97  |  PUNCT  |  .  |  punctuation mark, sentence closer\n",
      ".  |  97  |  PUNCT  |  .  |  punctuation mark, sentence closer\n",
      "Did  |  87  |  AUX  |  VBD  |  verb, past tense\n",
      "you  |  95  |  PRON  |  PRP  |  pronoun, personal\n",
      "made  |  100  |  VERB  |  VBD  |  verb, past tense\n",
      "it  |  95  |  PRON  |  PRP  |  pronoun, personal\n",
      "?  |  97  |  PUNCT  |  .  |  punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "sent1 = nlp(\"Nice borther!, and wow that was so profesional code write there!. Did you made it?\")\n",
    "# an example how to spacy can idenify the punctuation marks\n",
    "for token in sent1:\n",
    "    print(token, \" | \", token.pos, \" | \", token.pos_, \" | \", token.tag_, \" | \", spacy.explain(token.tag_))\n",
    "    #     word          id in pos         what is mean       tense of word       more explanaitin of tense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d5db1-019c-4155-b186-45b8cf87cd10",
   "metadata": {},
   "source": [
    "###### **POS using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df72a7fe-160b-4b96-9a43-26ea3c53c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Tags with NLTK\"] = {}\n",
    "words[\"Details with spacy\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d293e95-a918-49cc-9dc9-52adbb8dcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_nltk = nltk.pos_tag(nltk_word_tokens)\n",
    "details = [spacy.explain(tag) for _, tag in tag_nltk]\n",
    "# the details column that I have made came from spacy, this why its called *** with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d14b033-caab-465a-be1a-039be315a075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there also tag in spacy\n",
    "spacy.explain(text_spacy[1].tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bcde1071-b44a-451c-a39c-52807c867d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[\"Tags with NLTK\"] = [tag for _,tag in tag_nltk]\n",
    "words[\"Details with spacy\"] = details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b471c1ff-22f5-40bd-a79c-467a6b2e33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "      <th>Lemmatized words with SPacy</th>\n",
       "      <th>Lemmatized words with NLTK</th>\n",
       "      <th>Tags with Spacy</th>\n",
       "      <th>ID tags with Spacy</th>\n",
       "      <th>Tags with NLTK</th>\n",
       "      <th>Details with spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>In</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>95</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "      <td>100</td>\n",
       "      <td>VBP</td>\n",
       "      <td>verb, non-3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>93</td>\n",
       "      <td>CD</td>\n",
       "      <td>cardinal number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>84</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK Lemmatized words with SPacy  \\\n",
       "0           In          In                     in                          in   \n",
       "1         this        this                    thi                        this   \n",
       "2      lecture     lecture                 lectur                     lecture   \n",
       "3           we          we                     we                          we   \n",
       "4         take        take                   take                        take   \n",
       "..         ...         ...                    ...                         ...   \n",
       "207          2           2                      2                           2   \n",
       "208         of          of                     of                          of   \n",
       "209        the         the                    the                         the   \n",
       "210     hybrid      hybrid                 hybrid                      hybrid   \n",
       "211  procedure   procedure               procedur                   procedure   \n",
       "\n",
       "    Lemmatized words with NLTK Tags with Spacy  ID tags with Spacy  \\\n",
       "0                           In             ADP                  85   \n",
       "1                         this             DET                  90   \n",
       "2                      lecture            NOUN                  92   \n",
       "3                           we            PRON                  95   \n",
       "4                         take            VERB                 100   \n",
       "..                         ...             ...                 ...   \n",
       "207                          2             NUM                  93   \n",
       "208                         of             ADP                  85   \n",
       "209                        the             DET                  90   \n",
       "210                     hybrid             ADJ                  84   \n",
       "211                  procedure            NOUN                  92   \n",
       "\n",
       "    Tags with NLTK                                 Details with spacy  \n",
       "0               IN          conjunction, subordinating or preposition  \n",
       "1               DT                                         determiner  \n",
       "2               NN                             noun, singular or mass  \n",
       "3              PRP                                  pronoun, personal  \n",
       "4              VBP              verb, non-3rd person singular present  \n",
       "..             ...                                                ...  \n",
       "207             CD                                    cardinal number  \n",
       "208             IN          conjunction, subordinating or preposition  \n",
       "209             DT                                         determiner  \n",
       "210             JJ  adjective (English), other noun-modifier (Chin...  \n",
       "211             NN                             noun, singular or mass  \n",
       "\n",
       "[212 rows x 9 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4762e9c-12f9-4b41-922d-ff08164e0706",
   "metadata": {},
   "source": [
    "#### 4) NER: Named Entity Recognition\n",
    "\n",
    "\n",
    "Named Entity Recognition (NER) is the process of extracting entities and their types from text. In other words, it identifies predefined categories of objects within a body of text, such as:\n",
    "1) Locations\n",
    "2) Company names\n",
    "3) People name\n",
    "4) etc...\n",
    "\n",
    "Since Spacy is easier to use, we decided continue using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1a2cb-a9ec-41c8-8ef7-99b108975681",
   "metadata": {},
   "source": [
    "**NER using Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0bd02fc-2231-401c-8c50-e0aba4c0ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pd.DataFrame(data = {}, columns=[\"Entity\",\"NER with Spacy(Label)\", \"Label explained with Spacy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21502e5d-6c92-4ca0-89eb-3128e4709d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_entity = []\n",
    "spacy_label = []\n",
    "spacy_label_explained = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "527aa7a0-fd9f-47fa-b5ca-74715e53e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entitiy in text_spacy.ents:\n",
    "    spacy_entity.append(entitiy)\n",
    "    spacy_label.append(entitiy.label_)\n",
    "    spacy_label_explained.append(spacy.explain(entitiy.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2cef20b1-daa1-4cea-832d-c2f014451929",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner[\"Entity\"] = spacy_entity\n",
    "ner[\"NER with Spacy(Label)\"] = spacy_label\n",
    "ner[\"Label explained with Spacy\"] = spacy_label_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12c9b293-59a2-4240-8f7b-031443b18d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>NER with Spacy(Label)</th>\n",
       "      <th>Label explained with Spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(two)</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>Numerals that do not fall under another type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(first)</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>\"first\", \"second\", etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Cover)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1965)</td>\n",
       "      <td>DATE</td>\n",
       "      <td>Absolute or relative dates or periods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(second)</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>\"first\", \"second\", etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(RBF)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(only, three)</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>Numerals that do not fall under another type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(second)</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>\"first\", \"second\", etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1)</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>Numerals that do not fall under another type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2)</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>Numerals that do not fall under another type</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entity NER with Spacy(Label)  \\\n",
       "0          (two)              CARDINAL   \n",
       "1        (first)               ORDINAL   \n",
       "2        (Cover)                   ORG   \n",
       "3         (1965)                  DATE   \n",
       "4       (second)               ORDINAL   \n",
       "5          (RBF)                   ORG   \n",
       "6  (only, three)              CARDINAL   \n",
       "7       (second)               ORDINAL   \n",
       "8            (1)              CARDINAL   \n",
       "9            (2)              CARDINAL   \n",
       "\n",
       "                     Label explained with Spacy  \n",
       "0  Numerals that do not fall under another type  \n",
       "1                       \"first\", \"second\", etc.  \n",
       "2       Companies, agencies, institutions, etc.  \n",
       "3         Absolute or relative dates or periods  \n",
       "4                       \"first\", \"second\", etc.  \n",
       "5       Companies, agencies, institutions, etc.  \n",
       "6  Numerals that do not fall under another type  \n",
       "7                       \"first\", \"second\", etc.  \n",
       "8  Numerals that do not fall under another type  \n",
       "9  Numerals that do not fall under another type  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89dcfa-00e4-4961-be7a-74f3944f07d5",
   "metadata": {},
   "source": [
    "we can use this instead to make it as the fancy way as above in short line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "236bd376-71a1-4e4d-9e5e-1cb7e4b33c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> In this lecture we take a completely different approach Specifically we solve the problem of classifying nonlinearly separable patterns by proceeding in a hybrid manner involving \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " stages The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " stage transforms a given set of nonlinearly separable patterns into a new set for which under certain conditions the likelihood of the transformed patterns becoming linearly separable is high the mathematical justification of this transformation is traced to an early paper by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cover\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1965\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " stage completes the solution to the prescribed classification problem by using least squares estimation using a radial basis function \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RBF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " network the structure of which consists of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " layers The input layer is made up of source nodes sensory units that connect the network to its environment The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " layer consisting of hidden units applies a nonlinear transformation from the input space to the hidden feature space For most applications the dimensionality of the only hidden layer of the network is high this layer is trained in an unsupervised manner using stage \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the hybrid learning procedure The output layer is linear designed to supply the response of the network to the activation pattern applied to the input layer this layer is trained in a supervised manner using stage \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the hybrid procedure </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(text_spacy, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5af483ea-8f3d-45be-bc3c-6563457f9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL | Numerals that do not fall under another type\n",
      "DATE | Absolute or relative dates or periods\n",
      "EVENT | Named hurricanes, battles, wars, sports events, etc.\n",
      "FAC | Buildings, airports, highways, bridges, etc.\n",
      "GPE | Countries, cities, states\n",
      "LANGUAGE | Any named language\n",
      "LAW | Named documents made into laws.\n",
      "LOC | Non-GPE locations, mountain ranges, bodies of water\n",
      "MONEY | Monetary values, including unit\n",
      "NORP | Nationalities or religious or political groups\n",
      "ORDINAL | \"first\", \"second\", etc.\n",
      "ORG | Companies, agencies, institutions, etc.\n",
      "PERCENT | Percentage, including \"%\"\n",
      "PERSON | People, including fictional\n",
      "PRODUCT | Objects, vehicles, foods, etc. (not services)\n",
      "QUANTITY | Measurements, as of weight or distance\n",
      "TIME | Times smaller than a day\n",
      "WORK_OF_ART | Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp.pipe_labels[\"ner\"]:# what entity types that spacy support\n",
    "    print(ent, end = \" | \")\n",
    "    print(spacy.explain(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64640ead-a6c2-404c-a5be-8778c4ad4107",
   "metadata": {},
   "source": [
    "#### 5) Stop Words\n",
    "\n",
    "Stop words is certain noise that make the data hard to fit by the model, such as \"for, at, over and etc...\"\n",
    "\n",
    "so we need to remove them in order to make them less sparse or less space complexity. However this phase depends on the task/problem that you working on.\n",
    "\n",
    "But in general if you do the SWR phase in the preprocessing step, you will have an incorrect result for specific NLP problem.\n",
    "\n",
    "In other words:\n",
    "1) remove stop words in tasks where they add little value or introduce noise, like text classification, topic modeling, and information retrieval.\n",
    "2)  keep stop words in tasks where the structure and fluency of text are important, like text generation, machine translation, or named entity recognition.\n",
    "\n",
    "The timing for removing stop words is typically after tokenization but before feature extraction/engineering or modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e011f668-d026-4eca-aa80-5f97a8ddb57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stop words length:  326\n",
      "enough | go | ‘ve | former | our | part | 're | herself | n’t | if | why | else | at | again | nor | ’m | 's | others | so | whatever | but | indeed | ca | always | toward | because | seemed | front | many | you | n't | now | thereafter | well | each | when | together | beyond | him | whereupon | have | whenever | re | become | see | few | eleven | onto | much | several | once | only | then | or | it | out | and | latterly | full | 'll | anyway | meanwhile | during | 've | show | has | me | might | rather | myself | throughout | us | whom | twelve | for | be | would | until | ever | yourself | ’ll | where | call | nevertheless | beforehand | since | whereby | wherever | fifteen | we | other | thus | back | unless | two | no | are | anyone | which | name | something | most | really | very | almost | what | formerly | perhaps | will | move | up | serious | they | thereupon | four | behind | one | nowhere | another | own | ‘d | ’ve | say | beside | hereafter | thence | ourselves | itself | off | about | still | against | therein | whence | besides | their | is | such | quite | although | am | mine | within | i | top | becomes | yet | anyhow | though | while | somehow | take | keep | never | your | does | may | do | already | first | between | those | put | thereby | all | becoming | along | herein | themselves | namely | made | could | she | sixty | with | seem | third | whereafter | than | wherein | everyone | therefore | ’d | moreover | by | around | sometimes | side | five | someone | his | should | that | across | ‘re | regarding | latter | above | being | nine | hundred | without | next | amongst | alone | to | yours | used | bottom | cannot | please | various | often | seeming | however | anywhere | hence | everything | ‘m | ‘ll | was | himself | in | ours | everywhere | nobody | using | except | just | an | whither | make | due | noone | down | forty | mostly | whose | of | who | upon | anything | under | seems | whether | ’re | not | a | after | via | whole | through | empty | more | ‘s | twenty | below | from | here | thru | been | towards | least | fifty | same | her | n‘t | nothing | must | six | them | done | 'd | into | give | were | hers | on | did | ’s | ten | some | otherwise | per | the | none | both | as | became | whoever | every | get | afterwards | also | too | elsewhere | can | either | among | doing | before | less | my | last | these | 'm | even | over | how | neither | three | this | any | eight | amount | its | whereas | somewhere | hereupon | had | yourselves | there | he | further | hereby | sometime | "
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(\"The stop words length: \",len(STOP_WORDS))\n",
    "for sw in STOP_WORDS:\n",
    "    print(sw, end = \" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52ab222d-42b7-4b53-8de8-1e80fa4d62c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lecture completely different approach Specifically solve problem classifying nonlinearly separable patterns proceeding hybrid manner involving stages stage transforms given set nonlinearly separable patterns new set certain conditions likelihood transformed patterns linearly separable high mathematical justification transformation traced early paper Cover 1965 second stage completes solution prescribed classification problem squares estimation radial basis function RBF network structure consists layers input layer source nodes sensory units connect network environment second layer consisting hidden units applies nonlinear transformation input space hidden feature space applications dimensionality hidden layer network high layer trained unsupervised manner stage 1 hybrid learning procedure output layer linear designed supply response network activation pattern applied input layer layer trained supervised manner stage 2 hybrid procedure'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_words(text): \n",
    "    doc = nlp(text)\n",
    "    no_stop = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]# list comprehension \n",
    "    return \" \".join(no_stop)\n",
    "\n",
    "stop_words(text_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef3aeb-c4c0-4764-94e1-464d5f44ab2c",
   "metadata": {},
   "source": [
    "**to use the function above in dataframe no matter what the text size**\n",
    "\n",
    "but in this case, the dataframe contains single words in each row, so we will be using regex to fill spaces with nan, then drop them\n",
    "\n",
    "in case of long text in each row, it just using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5651cee-e89a-46c7-9dee-7ed2d959e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_no_stop = pd.DataFrame()\n",
    "\n",
    "words_with_no_stop = words[words[\"NLTK words\"].str.len()!=0] # give me the unempty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d381c46c-a591-4910-953d-783e12cae8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in words.columns: \n",
    "    if type(col) == (\"str\"):\n",
    "        words_with_no_stop[col] = words_with_no_stop[col].apply(stop_words)\n",
    "        words_with_no_stop.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3eb6ac70-5646-45c6-97c9-86bd456e4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_no_stop.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffc9eefa-c2e7-46cb-988c-55ec007fd2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK words</th>\n",
       "      <th>Spacy Words</th>\n",
       "      <th>Stemmed word with NLTK</th>\n",
       "      <th>Lemmatized words with SPacy</th>\n",
       "      <th>Lemmatized words with NLTK</th>\n",
       "      <th>Tags with Spacy</th>\n",
       "      <th>ID tags with Spacy</th>\n",
       "      <th>Tags with NLTK</th>\n",
       "      <th>Details with spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>In</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>In</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>thi</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lectur</td>\n",
       "      <td>lecture</td>\n",
       "      <td>lecture</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>95</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "      <td>100</td>\n",
       "      <td>VBP</td>\n",
       "      <td>verb, non-3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>93</td>\n",
       "      <td>CD</td>\n",
       "      <td>cardinal number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>85</td>\n",
       "      <td>IN</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>90</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>84</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedur</td>\n",
       "      <td>procedure</td>\n",
       "      <td>procedure</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>92</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLTK words Spacy Words Stemmed word with NLTK Lemmatized words with SPacy  \\\n",
       "0           In          In                     in                          in   \n",
       "1         this        this                    thi                        this   \n",
       "2      lecture     lecture                 lectur                     lecture   \n",
       "3           we          we                     we                          we   \n",
       "4         take        take                   take                        take   \n",
       "..         ...         ...                    ...                         ...   \n",
       "207          2           2                      2                           2   \n",
       "208         of          of                     of                          of   \n",
       "209        the         the                    the                         the   \n",
       "210     hybrid      hybrid                 hybrid                      hybrid   \n",
       "211  procedure   procedure               procedur                   procedure   \n",
       "\n",
       "    Lemmatized words with NLTK Tags with Spacy  ID tags with Spacy  \\\n",
       "0                           In             ADP                  85   \n",
       "1                         this             DET                  90   \n",
       "2                      lecture            NOUN                  92   \n",
       "3                           we            PRON                  95   \n",
       "4                         take            VERB                 100   \n",
       "..                         ...             ...                 ...   \n",
       "207                          2             NUM                  93   \n",
       "208                         of             ADP                  85   \n",
       "209                        the             DET                  90   \n",
       "210                     hybrid             ADJ                  84   \n",
       "211                  procedure            NOUN                  92   \n",
       "\n",
       "    Tags with NLTK                                 Details with spacy  \n",
       "0               IN          conjunction, subordinating or preposition  \n",
       "1               DT                                         determiner  \n",
       "2               NN                             noun, singular or mass  \n",
       "3              PRP                                  pronoun, personal  \n",
       "4              VBP              verb, non-3rd person singular present  \n",
       "..             ...                                                ...  \n",
       "207             CD                                    cardinal number  \n",
       "208             IN          conjunction, subordinating or preposition  \n",
       "209             DT                                         determiner  \n",
       "210             JJ  adjective (English), other noun-modifier (Chin...  \n",
       "211             NN                             noun, singular or mass  \n",
       "\n",
       "[212 rows x 9 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_with_no_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc82a4f-756f-4a0f-ad8d-23d82c0fd2cf",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We have many steps also as data preprocessing: \n",
    "1) Text Representation Using Label & One Hot Encoding (OHE), Bag Of Words (BOW), Bag of N-Gram, TF-IDF, Word Embeddings\n",
    "2) Word vectors\n",
    "\n",
    "\n",
    "In ML, **feature engineering** is the process of exctracting useful information (Features) from raw data, while in **NLP** its the process of converting/representing the text data into numerical data that algorithms can fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f1405-fd3d-43bc-a23b-49852e59c3ff",
   "metadata": {},
   "source": [
    "#### Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc42592-9b6d-43d6-a3c7-fbaf3964af3e",
   "metadata": {},
   "source": [
    "##### Label & OHE\n",
    "\n",
    "In NLP, no one uses these two approaches, but I'll show how to use them in context of converting the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986708f4-a307-4856-b2dc-2b2ba8859955",
   "metadata": {},
   "source": [
    "Some disadvantages of using **OHE & LE**\n",
    "- Explosion in feature space if number of categories are very high.\n",
    "- The vector representation of words is orthogonal and cannot determine or measure relationship between different words.\n",
    "- Cannot measure importance of a word in a sentence but understand presence/absence of a word in a sentence.\n",
    "- High dimensional sparse matrix representation can be memory & computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c40874d1-917a-4acf-8c39-de18b46bd825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# LabelEncoding object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# OHE object\n",
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fadca-6aef-4aef-8573-9fbc3908869f",
   "metadata": {},
   "source": [
    "##### Bag Of Words (BOW)\n",
    "BOW is as the name represents, is the putting the words in a bag and computes the frequency of occurrnece of each words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ed384417-0fd4-4be0-9d5d-554bed9cc017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>•In this lecture, we take a completely differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>•using a radial -basis function (RBF) network ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  •In this lecture, we take a completely differe...\n",
       "1  •using a radial -basis function (RBF) network ..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.DataFrame(data = text_list, columns=[\"Text\"])\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "344d6c22-a5a5-43a5-8b53-fc589a3b4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 0, 1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 4, 0, 0, 1, 0, 3, 1,\n",
       "       2, 0, 1, 0, 0, 0, 1, 0, 3, 2, 1, 1, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1,\n",
       "       0, 8, 2, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "raw_text_cv = (cv.fit_transform(raw[\"Text\"].values)).toarray()\n",
    "raw_text_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc318816-44a8-4a45-87b8-7ec1e23d213d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1965', 'activation', 'an', 'applications', 'applied', 'applies',\n",
       "       'approach', 'basis', 'becoming', 'by', 'certain', 'classification',\n",
       "       'classifying', 'completely', 'completes', 'conditions', 'connect',\n",
       "       'consisting', 'consists', 'cover', 'designed', 'different',\n",
       "       'dimensionality', 'early', 'environment', 'estimation', 'feature',\n",
       "       'first', 'for', 'from', 'function', 'given', 'hidden', 'high',\n",
       "       'hybrid', 'in', 'input', 'into', 'involving', 'is', 'its',\n",
       "       'justification', 'layer', 'layers', 'learning', 'least', 'lecture',\n",
       "       'likelihood', 'linear', 'linearly', 'made', 'manner',\n",
       "       'mathematical', 'most', 'network', 'new', 'nodes', 'nonlinear',\n",
       "       'nonlinearly', 'of', 'only', 'output', 'paper', 'pattern',\n",
       "       'patterns', 'prescribed', 'problem', 'procedure', 'proceeding',\n",
       "       'radial', 'rbf', 'response', 'second', 'sensory', 'separable',\n",
       "       'set', 'solution', 'solve', 'source', 'space', 'specifically',\n",
       "       'squares', 'stage', 'stages', 'structure', 'supervised', 'supply',\n",
       "       'take', 'that', 'the', 'this', 'three', 'to', 'traced', 'trained',\n",
       "       'transformation', 'transformed', 'transforms', 'two', 'under',\n",
       "       'units', 'unsupervised', 'up', 'using', 'we', 'which'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the vocabulary words for my text.\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62d7a0-3934-4852-8384-088286952d92",
   "metadata": {},
   "source": [
    "**Advantage of Count Vectorizer:**\n",
    "- CountVectorizer also gives us frequency of words in a text document/sentence which Onehot encoding fails to provide.\n",
    "- Length of the encoded vector is the length of the dictionary.\n",
    "  \n",
    "**Disadvantages of Count Vectorizer:**\n",
    "- This method ignores the location information of the word. It is not possible to grasp the meaning of a word from this representation.\n",
    "- The intuition that high-frequency words are more important or give more information\n",
    "about the sentence fails when it comes to stop words like “is, the, an, I” & when the corpus\n",
    "is context-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9d15e-0f66-4ac9-8540-c5ae3c0d21c0",
   "metadata": {},
   "source": [
    "##### Bag of N-grams\n",
    "\n",
    "since *BOW* just counting the number of ouccuerrence of each word, not caputring the meaning of these word, **N-gram** method comes to solve this issue in kernel like in **CNN**, hovering over 2 or 3 words in order to capture the meaning of the provided text.\n",
    "\n",
    "There also types of n-gram:\n",
    "- uni-gram is Bag of words, and we've talked about already.\n",
    "- bi-gram --> hovering over 2 words at a time to capture the meaning.\n",
    "- tri-gram --> as bi-gram but over 3 words.\n",
    "- and many grams you can use such as 4-gram, 5-gram and etc.. .\n",
    "\n",
    "However, there is some limitation of Bag of N-grams: \n",
    "- as N increases, the dimensionality increase\n",
    "- does not address out of vocabulary (OOV) problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "125104ee-236a-491d-93a1-f2e63d396dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 3,\n",
       " 'this': 6,\n",
       " 'lecture': 4,\n",
       " 'we': 7,\n",
       " 'take': 5,\n",
       " 'completely': 1,\n",
       " 'different': 2,\n",
       " 'approach': 0}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer() # using default n-gram which is uni-gram\n",
    "\n",
    "cv.fit([\"In this lecture, we take a completely different approach.\"])\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "985a8fb6-6791-4035-b5ca-2b94765deae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in this': 2,\n",
       " 'this lecture': 5,\n",
       " 'lecture we': 3,\n",
       " 'we take': 6,\n",
       " 'take completely': 4,\n",
       " 'completely different': 0,\n",
       " 'different approach': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2)) # to make it bi-gram\n",
    "\n",
    "cv.fit([\"In this lecture, we take a completely different approach.\"])\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3f5c6ac8-0866-449d-9796-bcec3dd28cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 6,\n",
       " 'this': 15,\n",
       " 'lecture': 9,\n",
       " 'we': 18,\n",
       " 'take': 12,\n",
       " 'completely': 1,\n",
       " 'different': 4,\n",
       " 'approach': 0,\n",
       " 'in this': 7,\n",
       " 'this lecture': 16,\n",
       " 'lecture we': 10,\n",
       " 'we take': 19,\n",
       " 'take completely': 13,\n",
       " 'completely different': 2,\n",
       " 'different approach': 5,\n",
       " 'in this lecture': 8,\n",
       " 'this lecture we': 17,\n",
       " 'lecture we take': 11,\n",
       " 'we take completely': 20,\n",
       " 'take completely different': 14,\n",
       " 'completely different approach': 3}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this output will be like uni-gram, then bi-gram, then tri-gram.\n",
    "cv13 = CountVectorizer(ngram_range=(1,3)) # uni-gram to tri gram\n",
    "\n",
    "\n",
    "cv13.fit([\"In this lecture, we take a completely different approach.\"])\n",
    "\n",
    "cv13.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019623a8-6315-44b8-83d3-a0a2eb238197",
   "metadata": {},
   "source": [
    "**I can use the list that I have made from lemmatization part, but I will make the whole pipelines in a single function.**\n",
    "\n",
    "[Lemmatization using Spacy](#Lemmatization-using-Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "724a46fc-f795-4506-949b-57104681d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ').replace('•', '').strip()\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or token.is_space: # stop words removal\n",
    "            continue\n",
    "        else: \n",
    "            processed_tokens.append(token.lemma_) # tokenization & Lemmatization \n",
    "    return \" \".join(processed_tokens)# here to convert the list into a full string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3594d8-0230-4b79-8702-b289eb1f8b58",
   "metadata": {},
   "source": [
    "you can see that am using the **text_list** object becuase I need a list to apply this function, however it can be applied on dataframe if its the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "192891a4-feb5-43a2-a501-c4ffac6a6488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lecture completely different approach specifically solve problem classify nonlinearly separable pattern proceed hybrid manner involve stage stage transform give set nonlinearly separable pattern new set certain condition likelihood transform pattern linearly separable high mathematical justification transformation trace early paper Cover 1965 second stage complete solution prescribed classification problem -square estimation',\n",
       " 'radial -basis function RBF network structure consist layer input layer source node sensory unit connect network environment second layer consist hide unit apply nonlinear transformation input space hide feature space application dimensionality hide layer network high layer train unsupervised manner stage 1 hybrid learn procedure output layer linear design supply response network activation pattern apply input layer layer train supervised manner stage 2 hybrid procedure']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed = [preprocess(txt) for txt in text_list] \n",
    "text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f6d19223-77ee-4cd5-ae18-dcb54ac3707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(ngram_range=(1, 2))</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "cv1.fit(text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c4e760c-6e55-4823-8391-c2f8bc8b18a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lecture': 77,\n",
       " 'completely': 21,\n",
       " 'different': 34,\n",
       " 'approach': 9,\n",
       " 'specifically': 147,\n",
       " 'solve': 140,\n",
       " 'problem': 114,\n",
       " 'classify': 17,\n",
       " 'nonlinearly': 101,\n",
       " 'separable': 132,\n",
       " 'pattern': 107,\n",
       " 'proceed': 119,\n",
       " 'hybrid': 56,\n",
       " 'manner': 85,\n",
       " 'involve': 63,\n",
       " 'stage': 151,\n",
       " 'transform': 167,\n",
       " 'give': 47,\n",
       " 'set': 135,\n",
       " 'new': 95,\n",
       " 'certain': 13,\n",
       " 'condition': 23,\n",
       " 'likelihood': 79,\n",
       " 'linearly': 83,\n",
       " 'high': 53,\n",
       " 'mathematical': 88,\n",
       " 'justification': 65,\n",
       " 'transformation': 170,\n",
       " 'trace': 162,\n",
       " 'early': 38,\n",
       " 'paper': 105,\n",
       " 'cover': 30,\n",
       " '1965': 0,\n",
       " 'second': 127,\n",
       " 'complete': 19,\n",
       " 'solution': 138,\n",
       " 'prescribed': 112,\n",
       " 'classification': 15,\n",
       " 'square': 149,\n",
       " 'estimation': 42,\n",
       " 'lecture completely': 78,\n",
       " 'completely different': 22,\n",
       " 'different approach': 35,\n",
       " 'approach specifically': 10,\n",
       " 'specifically solve': 148,\n",
       " 'solve problem': 141,\n",
       " 'problem classify': 115,\n",
       " 'classify nonlinearly': 18,\n",
       " 'nonlinearly separable': 102,\n",
       " 'separable pattern': 134,\n",
       " 'pattern proceed': 111,\n",
       " 'proceed hybrid': 120,\n",
       " 'hybrid manner': 58,\n",
       " 'manner involve': 86,\n",
       " 'involve stage': 64,\n",
       " 'stage stage': 154,\n",
       " 'stage transform': 155,\n",
       " 'transform give': 168,\n",
       " 'give set': 48,\n",
       " 'set nonlinearly': 137,\n",
       " 'pattern new': 110,\n",
       " 'new set': 96,\n",
       " 'set certain': 136,\n",
       " 'certain condition': 14,\n",
       " 'condition likelihood': 24,\n",
       " 'likelihood transform': 80,\n",
       " 'transform pattern': 169,\n",
       " 'pattern linearly': 109,\n",
       " 'linearly separable': 84,\n",
       " 'separable high': 133,\n",
       " 'high mathematical': 55,\n",
       " 'mathematical justification': 89,\n",
       " 'justification transformation': 66,\n",
       " 'transformation trace': 172,\n",
       " 'trace early': 163,\n",
       " 'early paper': 39,\n",
       " 'paper cover': 106,\n",
       " 'cover 1965': 31,\n",
       " '1965 second': 1,\n",
       " 'second stage': 129,\n",
       " 'stage complete': 152,\n",
       " 'complete solution': 20,\n",
       " 'solution prescribed': 139,\n",
       " 'prescribed classification': 113,\n",
       " 'classification problem': 16,\n",
       " 'problem square': 116,\n",
       " 'square estimation': 150,\n",
       " 'radial': 121,\n",
       " 'basis': 11,\n",
       " 'function': 45,\n",
       " 'rbf': 123,\n",
       " 'network': 90,\n",
       " 'structure': 156,\n",
       " 'consist': 27,\n",
       " 'layer': 67,\n",
       " 'input': 60,\n",
       " 'source': 142,\n",
       " 'node': 97,\n",
       " 'sensory': 130,\n",
       " 'unit': 173,\n",
       " 'connect': 25,\n",
       " 'environment': 40,\n",
       " 'hide': 49,\n",
       " 'apply': 6,\n",
       " 'nonlinear': 99,\n",
       " 'space': 144,\n",
       " 'feature': 43,\n",
       " 'application': 4,\n",
       " 'dimensionality': 36,\n",
       " 'train': 164,\n",
       " 'unsupervised': 176,\n",
       " 'learn': 75,\n",
       " 'procedure': 117,\n",
       " 'output': 103,\n",
       " 'linear': 81,\n",
       " 'design': 32,\n",
       " 'supply': 160,\n",
       " 'response': 125,\n",
       " 'activation': 2,\n",
       " 'supervised': 158,\n",
       " 'radial basis': 122,\n",
       " 'basis function': 12,\n",
       " 'function rbf': 46,\n",
       " 'rbf network': 124,\n",
       " 'network structure': 94,\n",
       " 'structure consist': 157,\n",
       " 'consist layer': 29,\n",
       " 'layer input': 69,\n",
       " 'input layer': 61,\n",
       " 'layer source': 73,\n",
       " 'source node': 143,\n",
       " 'node sensory': 98,\n",
       " 'sensory unit': 131,\n",
       " 'unit connect': 175,\n",
       " 'connect network': 26,\n",
       " 'network environment': 92,\n",
       " 'environment second': 41,\n",
       " 'second layer': 128,\n",
       " 'layer consist': 68,\n",
       " 'consist hide': 28,\n",
       " 'hide unit': 52,\n",
       " 'unit apply': 174,\n",
       " 'apply nonlinear': 8,\n",
       " 'nonlinear transformation': 100,\n",
       " 'transformation input': 171,\n",
       " 'input space': 62,\n",
       " 'space hide': 146,\n",
       " 'hide feature': 50,\n",
       " 'feature space': 44,\n",
       " 'space application': 145,\n",
       " 'application dimensionality': 5,\n",
       " 'dimensionality hide': 37,\n",
       " 'hide layer': 51,\n",
       " 'layer network': 72,\n",
       " 'network high': 93,\n",
       " 'high layer': 54,\n",
       " 'layer train': 74,\n",
       " 'train unsupervised': 166,\n",
       " 'unsupervised manner': 177,\n",
       " 'manner stage': 87,\n",
       " 'stage hybrid': 153,\n",
       " 'hybrid learn': 57,\n",
       " 'learn procedure': 76,\n",
       " 'procedure output': 118,\n",
       " 'output layer': 104,\n",
       " 'layer linear': 71,\n",
       " 'linear design': 82,\n",
       " 'design supply': 33,\n",
       " 'supply response': 161,\n",
       " 'response network': 126,\n",
       " 'network activation': 91,\n",
       " 'activation pattern': 3,\n",
       " 'pattern apply': 108,\n",
       " 'apply input': 7,\n",
       " 'layer layer': 70,\n",
       " 'train supervised': 165,\n",
       " 'supervised manner': 159,\n",
       " 'hybrid procedure': 59}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "126d5e82-5947-4074-8710-df31e15c7ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.transform([\"lecture completely different approach\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b4d4e-5b33-4aaa-a648-ed65169844ad",
   "metadata": {},
   "source": [
    "Here the vocabulary does not has this text, so we will see OOV problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "740d3c63-e83c-432e-bc0c-a9116302da90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.transform([\"Convolutional neural network\"]).toarray()\n",
    "\n",
    "\n",
    "# all zeros in the output below\n",
    "\n",
    "#the length of the list is the number of N-grams that I have determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844fc4d-2299-49a8-a332-c4ff62fb92df",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "the BOW method is simple and works well, but the problem with that is \n",
    "that it treats all words equally. As a result, it canno  distinguish ver \r\n",
    "common words or rare words. So, to solve this problem, TF-IDF comes in o\r\n",
    "the picture!\n",
    "\n",
    "Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<code>TF(t,d)</code> is a frequency term of a word to that document, in other words term frequency denotes the frequency of a word in a document.\n",
    "while <code>IDF (Inverse Document Frequency)</code>, it measures the importance of the word in the corpus."
   ]
  },
  {
   "attachments": {
    "f68c9263-2e0a-4217-8e50-ca8abaca23a7.png": {
     "image/png": "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"
    },
    "f70cc19c-b40d-44e7-b8a0-7af73a447811.png": {
     "image/png": "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"
    },
    "f7de863b-6fd9-469f-8cd2-dac027785896.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAA1CAYAAABcDmBpAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABvqSURBVHhe7d0FtFVF3wbw8bW7uxMLG1sRuxMLFcTu7vxU7A7EDsxlNyKI3d2Bga3YjaLub//GM77b68XXuvdwvPOsdRfn7LP39Dz/nM0IRYmQkZGRkdGw+E/t34yMjIyMBkUm8oyMjIwGRybyjIyMjAZHJvKMjIyMBkcm8oyMjIwGRybyjIyMjAZHJvKMjIyMBkcm8oyMjIwGRybyjIyMjAZHJvKMjIyMBkcm8oyIb775JuS3NWRkDBs//PBD3CfDIzKRZ4RXXnkl3HrrreHHH3+sXcnIyGiKL774Ipx77rlh6NChtSvDDzKRt3F8+OGH4eKLLw4zzjhjGGmkkWpXWxY//fRTePjhh8MDDzzwq7/HHnssfPnll7W7/h7eeuutsOaaa4aVV165dqV18MEHH4Rll102bL311q2ivbGinnrqqbDbbruFhRdeOPTo0SOO77CwzTbbxPa9+eabtSsZfxTjjz9+GGecccLZZ58dhgwZUrs6fGDE/ytR+5zRxsBU7NmzZ5h55plDp06dwggjjFD7pWUxePDgSLA0mwsvvDDcdddd4auvvgpnnXVWWHDBBcMUU0xRu/O3eOmllyJxzTDDDLUrzWPccccN7777bphkkklCx44da1dbHmONNVb46KOPwnjjjReWWmqp2tWWAy3xsMMOC4ssskjYaaedwrTTTvu74zfxxBOHp59+Oqy99tph9NFHr10dfvH555+H22+/PUw11VRh5JFHrl2tD+yPaaaZJtx4441hlFFG+Z9rsDWRNfI2jOeeey689tprUXP9z39abym8/fbbYdtttw1HH310JJ5NNtkkHHDAAWG99dYLk002WdQykTzt3B+B49onn3wSBQ9tHoEln/53330X7/v666/jZxqp5994440wxxxzxHsS/EZT/vbbb6MriQBJmnP6rgxI9/r3+++/j3W4J9WnXem6fxPef//9MPvss8f2qCe10780OW1Xru+eU6dy/ZY+V+E+dXpOmUnjdt/jjz8eXn755dC+ffs4lvPOO2/8rYrUL+00JtNPP30UdNB0nBN8Vpc6tcv3aj+1we/apgzlu8e9qaw0Tn5PSOWmslI5Tecjldu3b9/Qu3fv8PHHH/9SdypX23x2b2uBVr7CCiuEa6+99pd5GB6Q30feRmHDMcNnm222sNFGG9Wuti5sxqWXXjpq4lUCevXVV+PmJWjcs9Zaa4WNN9449OrVKxxzzDFhvvnmC5NOOmk49dRT4+/HHntsdGmMNtpoURPeYYcdoma88847hyOPPPJXmtMjjzwSLrrooigUbEjanhgBLYuAOPPMMyP503KfffbZcPDBB8cyWA1nnHFG2GeffSJRX3LJJWH99dcPE044YTjnnHOiRXPQQQfFcV1ppZWiC+PBBx+MfTnppJNiG7iPLr300lgf7e7kk08O/fv3j1bJ7rvvHvtw1VVXRdN91llnrbU4hIceeihcdtll4Z133omE5171IeXNNtssDBo0KCy00EJROC6xxBK1p34Gkrz88ssjIXKdcaVx+3Tu3Dm6n9Stn6wI7Va2Ppx++unxujqnm266OE8XXHBBtDLcY25YANxy9957b5xDc4RcXdtiiy0iwZpHAprrR7nuu++++2J9+uDPPdppnm+66abYH+PA+jKOyplzzjlDly5dwqKLLhr222+/8Omnn8a/ZZZZJuy6665h1FFHrfW45UF4dO3aNa4v7RkekDXyNgob9MUXXwzzzz9/7Urrg4tkjDHGiK6dKpAIbQxBIgDkh5i7desWBY/rNj/iPvHEE6P7RBCKZcH3O9FEE0UNjj/Tc1WMPfbYkXAGDhwYNTyb0cZUn3asssoq0ZwHZXE/KGPJJZcMs8wyS7j//vtjPYsvvni47bbbolCxqZEiDe2FF16IZDb11FOHI444IhKUupA0QvccgaBO2ueKK64YJp988vDEE0+EDTfcMBx++OHRfE9QJoFLuBBAyy+/fLjiiivibzRw5KY9559//m9IHJAs7fGEE06IAoLg00+aMWLWLsJLW6+++upIjnfccUe48847wyGHHBLrNF7G3dgk68Xa0U7zt9hii4Upp5wyPPnkk5GMteP666+PgmGDDTYIr7/+enxGQP3uu++OwnGXXXYJzz//fHSXrLPOOpG8fSYkjI1x4y5Sxo477hgF52qrrRbH2XPHHXdcOO+882KbWtvlQmhwDRKu2jo8IBN5G4UNZfPRtuoFmhkirJLtZ599Ftu27rrrRu0akfFHArKbYIIJYruBS4HWuNVWW4Uxxxwzank0N24D2qoN7noVNF0+ekS7+uqrRy3RNQRPa0ZQCAT4kpE3v7K/9957L1ov/LWIEAn7nXaPaEccccRIMkhfGUjONZowzdum5zpCAHPNNVckR33UZ4RMIHnWcwmPPvroL1YJAlFvNShNGGpDcxopMkSotG9tJrz1c6aZZoqEjKyNs/FKbgLlI3bkaYy0CTHrHzccFw5rwrizolgkfPLGBiEbJwJSewkaczLPPPPEtrCoaPRcaywgZLjAAgtE4WFOCSnzoQzfaeLGU3uT609fKR/bbbddjIFoV2u6BRPMvfXHuhoekIm8jYKLAYHRausBLgJZKok0E7gtEBltD6kgU+QMND7EnzYurZs2m0icxidzA9HoHyJprn/8ygiE5k6DVhdyRDaIUX208n79+kVfN4GAxJCd57SL2a/tPtO4ERwfrza6R3k2OhI2zsgLkXPltGvXLsgx0G73EGRzzz13rXW/Bk1V+7RVWQgUMSZwcRAIzQGhImzkp+5bbrklkqJ6tdVvCF6/CAztct1YdujQoVbKz9BPpEXwE5KEbWoz95HfjZt5Rcye969xMmc++zOOxrh79+7RNaU+c2wsCWztICAIQAKOkEzKBgvG81xdrBeavXvqAXPGX25+hgdkIm+DsOkQWD2j7giMtsXXXYUNYsPSmpEx4pNhAVwdCNJ1f8Dn634EgFCQEncCIqa9X3nllfG+Kp555plIfkhEmbQ/miuCQpYIndZMy0UoBIT2pvGifbqHoDCWPtMeuTyQHTcN7feoo46KhEVjJVBk5Bx44IGRlBG++hFxU9dSFbRlAgsJaiOSTdkw6lB3slCaA2JFdoSmMSL4aOLKI/D0xXVlI1bjq88ISl20c64i48pFQjhwyRAGBCrNmqWEbI034aA+ffSccSFE7rnnnjie22+/fbSgzJs6/OZfwkZ5iJoVoI3GRr36wLViDo4//vj4DOFm3v1eDxDuhKA1XK82VJHTD9sgkIiNKzjGtG9tID8BMQRlQ/ODJvBr03QEMm3cfffdN2rZwC0hCOY5pq3NjIiuu+66GLhEQMicya4O5MF1wPxPQAqIi7sBSSIDgUD+V2TCIqBtCtAlFwIfNJKnWTPltQtZr7rqqrFMmxkBaRMC81kQkQDi10dQtF3uFVk3NE+53MjQXNCw1d0c5PfT5gVGlWO7Jg2Vhq2/yLE5IE7jKaZAWAjMCdpylegLAcNfr48ChgSENhlXAWSWyxprrBEFViJ9AdxNN900fqdZG1+Ea470wdgYe8FR/STYCEu+cu4k7ScYafM0dcLMM9wq1qL5MP7aSPAREn369IlzKttGGwS8WUGHHnpo3ZQRc2Gt6AuXUtXdVQ/8q7JWaCc2pUViA7QWbGqLTb0WfUvANMl6QFqbb755JIa/ChqVIFbKZMhoTOy9996RrBFaRuuD622PPfaI+7Ia16gH6uJaIXWlEJHQaRGSyiL6tAbSWSbAgAEDonQHWtRyyy0XI+Q0HxqNYBWNBARJaHHMHKYZMMEERGhhLQlkyPeIHJn8LSEbaUA0KFrS3/XL0YJpqZnEGxPWF02e/zm5nTJaHyw6HJM4qp6oC5HzgwpoME+SWch3x/wTxLjhhhsikUtFYmaBABK/FJ8jc4ZWycRERjRiJqtymcwi80w2PksmHxO7JYFkBcVE6qXO8T+2BJpLp/sr4Ps0RhmNCevdHmCZCbJm1Ae0cBaReE69UbdgJ61QGlHyXwqi0KoRvMAQjZv7QN4uUkaOXCf8ZiBFSYqUYJlcWIRPQwflyJt1qEPghb8vBcf4Oh3c4AOVJ+t3/jaHSNzP7yUq7tCEAJG8XgLFPXKVpZ8J8BAODivI8dUu0F55x/yQrQEBPW2VuiaCL8cW+JDl2Qpe6aeTk8zABAG9TOSNC75nB5P49cUFMuoDJO4vnTuoJ+rmWhEo4SZJoCVyi4gEJwh60T6YklwvSBcJ0b6dVqPBI30mJm1VoAkMLjcMUvWeDQcd+K9lMyB7lgASdqjkmmuuiWaqyD83j9xm+bOi7+oSTHHwADHSfgRuHFJJZOgghr4A60AesCBUU2gjQuUWavrnuoDZn4FxcaydWeegCWEiMEionXbaaXE8CTNag3hBet8IU1DbBbUyMjL+OuwtHEMprTfqEuwUeZbQLxVL5BpojFKMROf5ngDJcVfQjpGuk2k0ddqw1C7uFuBC4Y4RnU95w4KPouvIzSkyUnPLLbeMmruTawJ9fOrKFCF3n1QrpFyNhKtXRJ/w4KZxJFjqmtNytHjpcT4nE1c7EKlTf/805NQ6ki3bRHyBti29TjaHdjsyrW6WhtN4si30ydFz1gQQUExy1pBATRXGFNFnZGT8FjJ4qjn8rHeuYPzit3qiLkTOzcEV4L0LSQNHIoKSrvM5y/VFOO5BnExJbhSEK39WQDMdzZXzi0Bpx4icK4ZA2HPPPaNrxnsoBD2lqHkWIQscIjqCgSUgvclzCDi9Fc7QeGeEqLQ6aLLeDSH9iduCH59LiBBiBcCwiJxGzg3TnPTmJtIvfvbfQ5XIpcFZRPJtmdfywi0o2rmgKw19r732iulerBFCB4yxcRWcbZrtgPCNQUZGxm/Bsk2JFEAhtQftM/xRVyDy1kZJhEVJkEVJNrUrRbHuuusWpWZbDBkypCg17mLppZcuSo05/laSb1FqzUX//v3j96YoybPo0qVLUWqb8bsySlIjoIqSOIuXX365GDRoUFFqsUUpEIqSrGKZpbCI97/99ttFScTFzjvvXJTEHK9BSf7FYostVnTs2LH44osvihNOOKEYc8wxY/tK0itKMixKYVG88cYbRUmQ8ZlrrrmmKK2I+Pmfxscff1yUkr+4/vrri88++6xYaaWV4pgNHTo0/rvKKqsUb731VnHjjTcW3bt3L0qtvDjllFOKr776qlbCzygtnKK0MmrfMjIy/goGDhxYdOjQIXJBvdHqPnK+YC4Uhwj4c4HG66ABXzQ3ipckOWhAGwaHR2joDrE0BwcYaKApDYgPXEYLH7bAJgtAtgdXBFeITBn+Y4FPoM1yKdByqyl5tHi+5vQuDimQpDINWnl88trleqrbAYimx5v/KThZ5/AGnz/QslkhNGwHZWjrTtdxs2g3K8Fnrqeqy0RfWBf1AitJjIIF0FqQpirl1ZpK894czLl7uKzK/VG72vbAFcm6SwH0pjCe8tjtzeEV3pujD38G+i3Bwf75X7Cn/CVXcF3xM5+3HmiutGF/5SDEazRzWjFt0t8nn3wStcwEmqjrJfHXrvwatOVu3boV/fr1q10ponaujtL8iXX+9NNP8V9atrpo6qkO9yq/aiEAzd79tF/wjDZ4riTu+F35yRIYPHhw0blz5+Kll16K3/9paL92vv/++7F+f9rjWupPSdBFucGK2267LV4vBWbU1B966KFaKUVxxBFHFOuvv37t26/Rt2/fon379lGjNw/KK03HYt555y1uuumm2l1/D+aTlXP33XfXrrQsSgFYrLbaasWAAQOiBWX8hgX9LQVjcdppp8U10wiw7tIa/afw5ptvxj1lDYG94FqCvdu1a9eiV69etSvNQ7tYxE2twtZAnz59ih133LH27Y+hVOqiJWuNAm4xvs2hVOCKdu3aFZ9++mntSv1QF9dKSwBh7bHHHtHc4TppbVioFnXPnj0jwdYLBEunTp2KJ598MrbD5lt11VWLJ554onZHUZxzzjnF8ssvP8x2LrzwwsUiiyxSlJZM/M5Vw1WVBO/fBcFH4HF3tTT00ZyU1krtyu+DMEf2SL8RQPBwlZmjlgIFobQCo4CrYtNNN/1dt4L1cuSRR0YX5GOPPVa72jjQb+7dHj161K78GhTH2Wef/TcKYD3wr3nXCneHdEUuBrnpMjlaC8wrGTMpgJqCpfUAtxL3AJPX+z6kU0rF9JfcRgK0grLykJseMOLucnqUuShFUZTeCVtZOQKk3DoCyQLQxtnBFHnzMn2Ysb57TYFAqgwhufXy+LmAnAGQJion333Gy/JTj+e1nWtK4Fc2kfkEAXCB3nJjRXO+tExi1k4V3E7qlF3kHIF2C5obA7n23n/iz/Xmcui582T7cPHJw5cxZU158ZOAuzYYF+mlguza4DyB6551oI3L8Oabb46uNUF37zGRMms9cOsIjumjoLSMB+8W0X/vYGla7v777x/TZQWqZVgJlOuz9e1/VjIf2uc9H37ncjN2zmEYC2Oofq41byVMfTaXguWyljzPXck1qGwvpOKGk37rHTHObphna9sZC+XKBnP62lyYI2mtxsC6SVlkCVyY1mFpLUYXpDolLJSWcqxLP2SgST6QDCBoqA/2LveGdaZM70q3XtPpac9Zn1KSjSs3rSSEKqwvrkfvZ9F3GWqyy6QsS6zwu7GqpuFKHDA33JNcqNogcUH5zpRYy8YMSu6MLuHSYokBz3qfkq7bgaCWgA1kM1r8rQm+aIeTpEoiqnpCbqu4ADKQOSPbhj86ZdVAOlUrNtAUFixS8dIjpG3D8jUaU5vMxgB+ZgIMiSAf93gG0cjisYmRhgwfsQm+R2QDNgzYFF6mhHz5JG0MbZVX7+CVOIYXRsksQsZO0HXv3v2XzZSArNTnPxlQFuLxsiVwhF16KUFyyimnRMKtQpYOAaTNiI+wKS2GeK5Am6TJ2rQIzr2EEuiXZ5zkleqJqJyLIESRvz/PIHFtIRgIBITl7X/SWo0dwhQz8BwgOP9LDvIiBJxzMFeIQr8Qrv67B9znlRZICsGJ29gDXoDlrIW5roIAkOVkXM2fNiLy9Ju+IkzkWlozUdBrL0FrPSBAn/VXOQg8nckgqKrwnP5LAybslW+OvSTNWRJzStEwL+ZWveacMDUPYlaEhxgVgWMOnQOxRqXOGhNCxxxYm1UkQWXdE+DmRnxM7EMMhFBuuv7xh7Wc3sgpXue1IQS0k+SEWoKxMQYy4upN4vCvIvKMPwaLm3Zhk1QhDVMA0lvsLFBao01OayEgbCyEbsPaJAKmNrJrNj4tlPaDeP1rgUvLsjFs+vS6VcFn96iDVs5CsIFob6wEKakOadH2EBIhgMBsKsICcVWB1LTN74iMtpZek4CUBcJpWc2BRkXLpA16VjsRFQLTBuXQuAho12mXSBcx0fg8Qyv2WZ9ZAciL1kkIsgCMgXL0S7/d43Sm3GPlul9aG3LVFsLD/cYFIRGsviNl7+02T+nlbAjM3CSNG2jgLAPv+Xbq1xgnuNc8EMQSD7TFPIMAppRa6yMRKyGCCI2hNZNAIFkD+ob8zXHTMba+jG965zzyI8yMDeGOxK0h64Cgo2Hrh+/SewlN1425djrDYY1Zc8bEGRTWkzkzD1WYd/VpozFk3bmPENFW9SSLL0E9hLV5Bm0jkJuWDebWO4+SYKw3MpG3UTBZaSTIIsGitUBtdJuWBiKTyKJPJqhNbbHLCrJJkWw6C0BTov0ySxGBjY3IfEaMTGabxQZHfjaAZ9zPrUBLSi4O5EOjQ1o2PJdVlayqoOmrxyZHEjRZZQLSpXlW83+rQGjapj/6TmNUlrYZH2SmXqRAsya0mNrKoylW4TqN3/365XMaN23k3kIe+k/jNB7Iy1h7hmWhPQ7KsaT03zwhWuRKsCF0c5DcGMrVjuQiM4f++zrjy9WUXk2RQIBwiSE6mi+XBLeVvhIo+q6viDq9Jx3BGx9CADxLCHiNr2fUgayrVh+YT/OXXqth3dBigdWoThYU4aZOrjhaMfJN0B8CyTgS1oSrMtWHaJVB2276Ombjqm2pLGOrHc5qEJgEQ/UUOegnDT/NGRcgpSC9RqQKY2DtJwu13shE3kaBcBEC4kuwwGleSAGx0s5oiAjZpgAb0WcaNHPevTZU0lJtFpvEhrP5aTPKRbJcFcgDmSM0pOV1BlwiNg9NyMb2X4LZqIiDEECQNP5hQXto/zY91w4BZEOC9iLMptpXAnJRN7Ln4iA0EDBtizaK0JEMNwjBhbxooOokBPU7kRPCQSj6Zdz0xxjrAyFAU0ZCyJcmZ4wRTLqPu0H53Ahev0BYGEsEg+hp7z67hpBppohMe9WJ8JG3+rggCDNjV4U6U2xDn2moLCwuB5/1V9+NizlOggOhKddn1/Rbf5RhzBCkNhirBPdaA8YL8ROU6ufu8jpm/m3rw9rwu/VBqCUiBX3UHmNEuIsbsEYIGQLIc9Yc0tWWBONiDSB0bTK3LBr9I2CUYe3rZ4J6ED9Bi6TNq+/KJdiqIPDNTxJ29UYm8jYKmoSFnvzWwEeMCNOipelYyMnEBlqMZ/glmbY2A1Kw8L2HJr1ywT1cNAjWRkUoiN79SJr/0WnddLoWMXKf0MZpovzrNr77aXrJLdMcmN98mAQPISFgZcMCAqWxakNzMA6ICanY8EifX5pQENRDWHy8/M1e1eB3ggKJ8I17BYLgKjjPgKDEJggebiqaOaJEitqBOAg244iQkZa+Gi+EhLi4FYxDErLiD0hZHwg3wpZvW72IhODjPzdeBBaNXJzBdwK7KcypZ5PbxX008eRCICS0mTAyJubfOApqum5sgNDSB/9aN9rue4JxMraCi4gWiYsJEPL+5dM3FqD/+qauqs9ZmQSEsSE4kbA5SJaPNUyT52Kq1k3BEEjVbmsTSad4gee4DJExwZqgfoqG36x3bRHP0W9WXoI5JGy9aTVZQvXGv+o/lsj4c7BIkZZgn4WLICxSCzptJtcQSCJGC981v9vwCMoG9Zm56VkbClF4xm9Ju3OP35RhU/qMwFPZCCWRIvJ2v2dpRModFlL5yk3PKVsdXAcyQ7g1moPlr05lJC07adzgN+1KAsk4+V19SE7/XFdfar/69cln7fHZ/WlsjFO6rj5/ylCH35TrtzTu+u855eqje9yrjFRnaofv7tdG97vHs1W4x18a06Zz7FnWgTI9r07tV051XDxjPNSnzX5LAh9S21xTjjZX5yr1IY2DcpSp3gTlut99+qOtoO1pHtJ8pfZDGodUr/tSe1OZTetK7U3XtdVYe6baN64kGTWCr9Xn64lM5G0YFq00Nf5UgcJ/G5jhUsh69+79i680I+PvgCCRWcO6cvp8eEF2rbRh0DxkW8jR5Rb5N4EJ7tXD3DSZxDP+KXDH0NSbZk7VG1kjb+Mw/Xy5Anny4KvmaUZGxn/B3eSAkPjOsILn9UIm8ozoMxRUEkCq+jgzMjL+C/5ygV7B1uENmcgzMjIyGhxZ/crIyMhocGQiz8jIyGhwZCLPyMjIaHBkIs/IyMhocGQiz8jIyGhwZCLPyMjIaHBkIs/IyMhocGQiz8jIyGhohPD/kDtblRvDruYAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "d1bf3ac2-b1a8-4968-b2ce-ed8551756544",
   "metadata": {},
   "source": [
    "![Screenshot 2024-09-29 085307.png](attachment:f70cc19c-b40d-44e7-b8a0-7af73a447811.png)\n",
    "![Screenshot 2024-09-29 085319.png](attachment:f7de863b-6fd9-469f-8cd2-dac027785896.png)\n",
    "![Screenshot 2024-09-29 085334.png](attachment:f68c9263-2e0a-4217-8e50-ca8abaca23a7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68d53c-7130-4d87-b16e-f6fece4e8446",
   "metadata": {},
   "source": [
    "**Important Points about TF-IDF Vectorizer**\n",
    "1. Similar to the count vectorization method, in the TF-IDF method, a document term matrix is generated and each\n",
    "column represents an individual unique word.\n",
    "2. The difference in the TF-IDF method is that each cell doesn’t indicate the term frequency, but contains a weight\n",
    "value that signifies how important a word is for an individual text message or document\n",
    "3. This method is based on the frequency method but it is different from the count vectorization in the sense that it\n",
    "takes into considerations not just the occurrence of a word in a single document but in the entire corpus.\n",
    "4. TF-IDF gives more weight to less frequently occurring events and less weight to expected events. So, it penalizes\n",
    "frequently occurring words that appear frequently in a document such as “the”, “is” but assigns greater weight to\n",
    "less frequent or rare words.\n",
    "5. The product of TF x IDF of a word indicates how often the token is found in the document and how unique the\n",
    "token is to the whole entire corpus of documents.\n",
    "\n",
    "\n",
    "**Some disadvantages about TF-IDF Vectorizer**\n",
    "1. does not address out of vocabulary (OOV) problem.\n",
    "2. does not capture the relation between the words.\n",
    "3. as the words increases, the dimensionality also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4ba0a6b-45a3-46ef-adbf-ddce58a87db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfV = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc0daecf-0e34-4da5-aae2-572abf10bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = tfV.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1c96818-6c90-4edf-a08b-51c2212a6523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in': 35, 'this': 90, 'lecture': 46, 'we': 104, 'take': 87, 'completely': 13, 'different': 21, 'approach': 6, 'specifically': 80, 'solve': 77, 'the': 89, 'problem': 66, 'of': 59, 'classifying': 12, 'nonlinearly': 58, 'separable': 74, 'patterns': 64, 'by': 9, 'proceeding': 68, 'hybrid': 34, 'manner': 51, 'involving': 38, 'two': 98, 'stages': 83, 'first': 27, 'stage': 82, 'transforms': 97, 'given': 31, 'set': 75, 'into': 37, 'new': 55, 'for': 28, 'which': 105, 'under': 99, 'certain': 10, 'conditions': 15, 'likelihood': 47, 'transformed': 96, 'becoming': 8, 'linearly': 49, 'is': 39, 'high': 33, 'mathematical': 52, 'justification': 41, 'transformation': 95, 'traced': 93, 'to': 92, 'an': 2, 'early': 23, 'paper': 62, 'cover': 19, '1965': 0, 'second': 72, 'completes': 14, 'solution': 76, 'prescribed': 65, 'classification': 11, 'using': 103, 'least': 45, 'squares': 81, 'estimation': 25, 'radial': 69, 'basis': 7, 'function': 30, 'rbf': 70, 'network': 54, 'structure': 84, 'consists': 18, 'only': 60, 'three': 91, 'layers': 43, 'input': 36, 'layer': 42, 'made': 50, 'up': 102, 'source': 78, 'nodes': 56, 'sensory': 73, 'units': 100, 'that': 88, 'connect': 16, 'its': 40, 'environment': 24, 'consisting': 17, 'hidden': 32, 'applies': 5, 'nonlinear': 57, 'from': 29, 'space': 79, 'feature': 26, 'most': 53, 'applications': 3, 'dimensionality': 22, 'trained': 94, 'unsupervised': 101, 'learning': 44, 'procedure': 67, 'output': 61, 'linear': 48, 'designed': 20, 'supply': 86, 'response': 71, 'activation': 1, 'pattern': 63, 'applied': 4, 'supervised': 85}\n"
     ]
    }
   ],
   "source": [
    "print(tfV.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e75349ce-4bff-460f-b2d4-0ebba38e0719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfV.vocabulary_.get(\"supply\") # 'supply': 86 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f471e5a-4104-4fc2-a730-509e0550bdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1965 : 1.4054651081081644\n",
      "1 activation : 1.4054651081081644\n",
      "2 an : 1.0\n",
      "3 applications : 1.4054651081081644\n",
      "4 applied : 1.4054651081081644\n",
      "5 applies : 1.4054651081081644\n",
      "6 approach : 1.4054651081081644\n",
      "7 basis : 1.4054651081081644\n",
      "8 becoming : 1.4054651081081644\n",
      "9 by : 1.4054651081081644\n",
      "10 certain : 1.4054651081081644\n",
      "11 classification : 1.4054651081081644\n",
      "12 classifying : 1.4054651081081644\n",
      "13 completely : 1.4054651081081644\n",
      "14 completes : 1.4054651081081644\n",
      "15 conditions : 1.4054651081081644\n",
      "16 connect : 1.4054651081081644\n",
      "17 consisting : 1.4054651081081644\n",
      "18 consists : 1.4054651081081644\n",
      "19 cover : 1.4054651081081644\n",
      "20 designed : 1.4054651081081644\n",
      "21 different : 1.4054651081081644\n",
      "22 dimensionality : 1.4054651081081644\n",
      "23 early : 1.4054651081081644\n",
      "24 environment : 1.4054651081081644\n",
      "25 estimation : 1.4054651081081644\n",
      "26 feature : 1.4054651081081644\n",
      "27 first : 1.4054651081081644\n",
      "28 for : 1.0\n",
      "29 from : 1.4054651081081644\n",
      "30 function : 1.4054651081081644\n",
      "31 given : 1.4054651081081644\n",
      "32 hidden : 1.4054651081081644\n",
      "33 high : 1.0\n",
      "34 hybrid : 1.0\n",
      "35 in : 1.0\n",
      "36 input : 1.4054651081081644\n",
      "37 into : 1.4054651081081644\n",
      "38 involving : 1.4054651081081644\n",
      "39 is : 1.0\n",
      "40 its : 1.4054651081081644\n",
      "41 justification : 1.4054651081081644\n",
      "42 layer : 1.4054651081081644\n",
      "43 layers : 1.4054651081081644\n",
      "44 learning : 1.4054651081081644\n",
      "45 least : 1.4054651081081644\n",
      "46 lecture : 1.4054651081081644\n",
      "47 likelihood : 1.4054651081081644\n",
      "48 linear : 1.4054651081081644\n",
      "49 linearly : 1.4054651081081644\n",
      "50 made : 1.4054651081081644\n",
      "51 manner : 1.0\n",
      "52 mathematical : 1.4054651081081644\n",
      "53 most : 1.4054651081081644\n",
      "54 network : 1.4054651081081644\n",
      "55 new : 1.4054651081081644\n",
      "56 nodes : 1.4054651081081644\n",
      "57 nonlinear : 1.4054651081081644\n",
      "58 nonlinearly : 1.4054651081081644\n",
      "59 of : 1.0\n",
      "60 only : 1.4054651081081644\n",
      "61 output : 1.4054651081081644\n",
      "62 paper : 1.4054651081081644\n",
      "63 pattern : 1.4054651081081644\n",
      "64 patterns : 1.4054651081081644\n",
      "65 prescribed : 1.4054651081081644\n",
      "66 problem : 1.4054651081081644\n",
      "67 procedure : 1.4054651081081644\n",
      "68 proceeding : 1.4054651081081644\n",
      "69 radial : 1.4054651081081644\n",
      "70 rbf : 1.4054651081081644\n",
      "71 response : 1.4054651081081644\n",
      "72 second : 1.0\n",
      "73 sensory : 1.4054651081081644\n",
      "74 separable : 1.4054651081081644\n",
      "75 set : 1.4054651081081644\n",
      "76 solution : 1.4054651081081644\n",
      "77 solve : 1.4054651081081644\n",
      "78 source : 1.4054651081081644\n",
      "79 space : 1.4054651081081644\n",
      "80 specifically : 1.4054651081081644\n",
      "81 squares : 1.4054651081081644\n",
      "82 stage : 1.0\n",
      "83 stages : 1.4054651081081644\n",
      "84 structure : 1.4054651081081644\n",
      "85 supervised : 1.4054651081081644\n",
      "86 supply : 1.4054651081081644\n",
      "87 take : 1.4054651081081644\n",
      "88 that : 1.4054651081081644\n",
      "89 the : 1.0\n",
      "90 this : 1.0\n",
      "91 three : 1.4054651081081644\n",
      "92 to : 1.0\n",
      "93 traced : 1.4054651081081644\n",
      "94 trained : 1.4054651081081644\n",
      "95 transformation : 1.0\n",
      "96 transformed : 1.4054651081081644\n",
      "97 transforms : 1.4054651081081644\n",
      "98 two : 1.4054651081081644\n",
      "99 under : 1.4054651081081644\n",
      "100 units : 1.4054651081081644\n",
      "101 unsupervised : 1.4054651081081644\n",
      "102 up : 1.4054651081081644\n",
      "103 using : 1.0\n",
      "104 we : 1.4054651081081644\n",
      "105 which : 1.0\n"
     ]
    }
   ],
   "source": [
    "# to get the score of each index(word) (idf score)\n",
    "all_features_name = tfV.get_feature_names_out()\n",
    "\n",
    "for word in all_features_name:# you can use (tfV.get_feature_names_out()) instead of the all_features_name object\n",
    "    index = tfV.vocabulary_.get(word) # vocabulary_.get() is method to get the index of a certain word\n",
    "    print(f\"{index} {word} : {tfV.idf_[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca39b2b6-02ae-4ac0-9e90-9b997c17d50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08569242, 0.        , 0.06097086, 0.        , 0.        ,\n",
       "        0.        , 0.08569242, 0.        , 0.08569242, 0.25707725,\n",
       "        0.08569242, 0.08569242, 0.08569242, 0.08569242, 0.08569242,\n",
       "        0.08569242, 0.        , 0.        , 0.        , 0.08569242,\n",
       "        0.        , 0.08569242, 0.        , 0.08569242, 0.        ,\n",
       "        0.08569242, 0.        , 0.08569242, 0.06097086, 0.        ,\n",
       "        0.        , 0.08569242, 0.        , 0.06097086, 0.06097086,\n",
       "        0.12194172, 0.        , 0.08569242, 0.08569242, 0.12194172,\n",
       "        0.        , 0.08569242, 0.        , 0.        , 0.        ,\n",
       "        0.08569242, 0.08569242, 0.08569242, 0.        , 0.08569242,\n",
       "        0.        , 0.06097086, 0.08569242, 0.        , 0.        ,\n",
       "        0.08569242, 0.        , 0.        , 0.17138483, 0.24388344,\n",
       "        0.        , 0.        , 0.08569242, 0.        , 0.25707725,\n",
       "        0.08569242, 0.17138483, 0.        , 0.08569242, 0.        ,\n",
       "        0.        , 0.        , 0.06097086, 0.        , 0.25707725,\n",
       "        0.17138483, 0.08569242, 0.08569242, 0.        , 0.        ,\n",
       "        0.08569242, 0.08569242, 0.12194172, 0.08569242, 0.        ,\n",
       "        0.        , 0.        , 0.08569242, 0.        , 0.48776688,\n",
       "        0.12194172, 0.        , 0.12194172, 0.08569242, 0.        ,\n",
       "        0.06097086, 0.08569242, 0.08569242, 0.08569242, 0.08569242,\n",
       "        0.        , 0.        , 0.        , 0.06097086, 0.17138483,\n",
       "        0.06097086],\n",
       "       [0.        , 0.05325217, 0.03788936, 0.05325217, 0.05325217,\n",
       "        0.05325217, 0.        , 0.05325217, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05325217, 0.05325217, 0.05325217, 0.        ,\n",
       "        0.05325217, 0.        , 0.05325217, 0.        , 0.05325217,\n",
       "        0.        , 0.05325217, 0.        , 0.03788936, 0.05325217,\n",
       "        0.05325217, 0.        , 0.15975651, 0.03788936, 0.07577872,\n",
       "        0.07577872, 0.15975651, 0.        , 0.        , 0.18944679,\n",
       "        0.05325217, 0.        , 0.3727652 , 0.05325217, 0.05325217,\n",
       "        0.        , 0.        , 0.        , 0.05325217, 0.        ,\n",
       "        0.05325217, 0.07577872, 0.        , 0.05325217, 0.21300869,\n",
       "        0.        , 0.05325217, 0.05325217, 0.        , 0.34100423,\n",
       "        0.10650434, 0.05325217, 0.        , 0.05325217, 0.        ,\n",
       "        0.        , 0.        , 0.10650434, 0.        , 0.05325217,\n",
       "        0.05325217, 0.05325217, 0.03788936, 0.05325217, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05325217, 0.10650434,\n",
       "        0.        , 0.        , 0.07577872, 0.        , 0.05325217,\n",
       "        0.05325217, 0.05325217, 0.        , 0.05325217, 0.60622973,\n",
       "        0.07577872, 0.05325217, 0.18944679, 0.        , 0.10650434,\n",
       "        0.03788936, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.10650434, 0.05325217, 0.05325217, 0.11366808, 0.        ,\n",
       "        0.03788936]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the TF-IDF: \n",
    "vectorized_text.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bd5a6-6e4f-4fbe-b075-7d513c761af9",
   "metadata": {},
   "source": [
    "##### Word Embeddings\n",
    "\n",
    "Since *tf-idf* does not caputre the meaning of the words, **word embeddings** came to solve that issue.\n",
    "\n",
    "**Word Embedding** is a learned representation for text where words that have the same meaning have a similar\n",
    "representation. \n",
    "\n",
    "numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word\n",
    "vectors can capture the connotation of words.\n",
    "\n",
    "\n",
    "**Important points of Words Embeddings**\n",
    "1. Similar words, have similar vectors\n",
    "2. Dimensionality are low\n",
    "\n",
    "                                               **Word Embedding techniques**\n",
    "\n",
    "**Built on CBOW (Continous Bag Of Words), Skip Gram**\n",
    "\n",
    "- CBOW-> given a context word, to predict target word\n",
    "- Skip gram-> given a target word, to predict context word. In this tech, the word embedding between the input layer & hidden layer, the c bar between the wieght layer and the output layer\n",
    "\n",
    "1. Word2vec\n",
    "2. GloVe\n",
    "3. fastText\n",
    "\n",
    "\n",
    "\n",
    "**Based on Advanced techniques in NLP built on Transformers** \n",
    "1. Bert\n",
    "2. GPT\n",
    "\n",
    "**Based on LSTM**\n",
    "1. ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca709c6-47ec-4ac5-9ca8-a321ab02b0c8",
   "metadata": {},
   "source": [
    "##### Word Vector (Word embedding using Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebd1e0-7d8e-4082-9beb-0faaa46346ca",
   "metadata": {},
   "source": [
    "before training any alogrithm, we need to do some preprocess steps as we've been doing above and they include: \n",
    "- lower case all text\n",
    "- removing punctuations\n",
    "- etc...\n",
    "\n",
    "However, gensim has a function that do all the thing which is <code>gensim.utils.simple_preprocess</code>  and it make the text as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "47ddb79c-a0f7-4f95-9f5a-19bccdebac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22f00f64-c13c-457a-b60a-72d704371278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this lecture we take a completel'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned[:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "594aa28c-3404-42df-9bd9-3a3d6c6f4fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'this', 'lecture', 'we', 'take', 'completel']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gensim = simple_preprocess(text_cleaned[:36])\n",
    "text_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "adfe1088-4291-4e4a-af99-5f237dadbdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_gensim = simple_preprocess(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5c690e1-0fcd-422d-a430-50d13380b56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              In\n",
       "1            this\n",
       "2         lecture\n",
       "3              we\n",
       "4            take\n",
       "5               a\n",
       "6      completely\n",
       "7       different\n",
       "8        approach\n",
       "9    Specifically\n",
       "Name: NLTK words, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[\"NLTK words\"].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1307c04c-3a95-4493-b9c6-c3395a4d46a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [in]\n",
       "1            [this]\n",
       "2         [lecture]\n",
       "3              [we]\n",
       "4            [take]\n",
       "5                []\n",
       "6      [completely]\n",
       "7       [different]\n",
       "8        [approach]\n",
       "9    [specifically]\n",
       "Name: NLTK words, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genism_10words = words[\"NLTK words\"].iloc[:10].apply(simple_preprocess)\n",
    "genism_10words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "917ed9a8-d891-4032-b8cb-4796e1de5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(\n",
    "    window=5, # means 5 words before the target word and 10 after it\n",
    "    min_count = 2,\n",
    "    workers=8,\n",
    ")\n",
    "\n",
    "model.build_vocab(text_gensim, progress_per=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21b0c584-5861-4c40-86b4-19b00e6a0e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs # the 5 is a default number of the times of iterations on the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "473cdd1d-e989-4e93-bf1c-852154ba67d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "59f8e5af-e7aa-4127-b9e2-71bb11c8af6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 6875)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "791dc685-f4c6-45dc-8b0a-16981f69d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./lecture-8-model-pretrained.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5ca18-0888-4fa1-8c95-9af83a3a5a2d",
   "metadata": {},
   "source": [
    "Since we're dealing with single text, the model cant learn the words together since each word is a single vector, \n",
    "word2Vec works as window of the text, so when we do a tokenization of that single text into words, each word will be a single vector of its own, so the model will not capture anything, in the other hand, if the text is list of multiple texts, the model will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "363cad71-4276-41d0-a259-7e42bf707846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 0.2529045641422272),\n",
       " ('b', 0.14256367087364197),\n",
       " ('t', 0.13725271821022034),\n",
       " ('g', 0.1166219636797905),\n",
       " ('k', 0.07191766053438187),\n",
       " ('o', 0.04410674050450325),\n",
       " ('u', 0.027008363977074623),\n",
       " ('r', 0.012811624445021152),\n",
       " ('i', 0.006598467472940683),\n",
       " ('p', -0.0011978144757449627)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49508656-7a8f-4b60-bdba-4de599fa97d8",
   "metadata": {},
   "source": [
    "To get a vector of a certain word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5d13c2d4-d189-45cf-a28e-60c6bb4125c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00950012,  0.00956222, -0.00777076, -0.00264551, -0.00490641,\n",
       "       -0.0049667 , -0.00802359, -0.00778358, -0.00455321, -0.00127536,\n",
       "       -0.00510299,  0.00614054, -0.00951662, -0.0053071 ,  0.00943715,\n",
       "        0.00699133,  0.00767582,  0.00423474,  0.00050709, -0.00598114,\n",
       "        0.00601878,  0.00263503,  0.00769943,  0.00639384,  0.00794257,\n",
       "        0.00865741, -0.00989575, -0.0067557 ,  0.00133757,  0.0064403 ,\n",
       "        0.00737382,  0.00551698,  0.00766163, -0.00512557,  0.00658441,\n",
       "       -0.00410837, -0.00905534,  0.00914168,  0.0013314 , -0.00275968,\n",
       "       -0.00247784, -0.00422048,  0.00481234,  0.00440022, -0.00265336,\n",
       "       -0.00734188, -0.00356585, -0.00033661,  0.00609589, -0.00283734,\n",
       "       -0.00012089,  0.00087973, -0.00709565,  0.002065  , -0.00143242,\n",
       "        0.00280215,  0.00484222, -0.00135202, -0.00278014,  0.00773865,\n",
       "        0.0050456 ,  0.00671352,  0.00451564,  0.00866716,  0.00747497,\n",
       "       -0.00108189,  0.00874764,  0.00460172,  0.00544063, -0.00138608,\n",
       "       -0.00204132, -0.00442435, -0.0085152 ,  0.00303773,  0.00888319,\n",
       "        0.00891974, -0.00194235,  0.00608616,  0.00377972, -0.00429597,\n",
       "        0.00204292, -0.00543789,  0.00820889,  0.00543291,  0.00318443,\n",
       "        0.00410257,  0.00865715,  0.00727203, -0.00083347, -0.00707277,\n",
       "        0.00838047,  0.00723358,  0.00173047, -0.00134749, -0.00589009,\n",
       "       -0.00453309,  0.00864797, -0.00313511, -0.00633882,  0.00987008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"f\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f84d6-c280-4a1a-bc3e-f924a82c2636",
   "metadata": {},
   "source": [
    "##### Word Vector (Word embedding using Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0efc5e65-7bd3-4c55-9972-fb1136524e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "503502a3-9812-4685-b559-8519d798068a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:    Vector: False  OOV: True\n",
      "Token: In  Vector: True  OOV: False\n",
      "Token: this  Vector: True  OOV: False\n",
      "Token: lecture  Vector: True  OOV: False\n",
      "Token: we  Vector: True  OOV: False\n",
      "Token: take  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: completely  Vector: True  OOV: False\n",
      "Token: different  Vector: True  OOV: False\n",
      "Token: approach  Vector: True  OOV: False\n",
      "Token: Specifically  Vector: True  OOV: False\n",
      "Token: we  Vector: True  OOV: False\n",
      "Token: solve  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: problem  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: classifying  Vector: True  OOV: False\n",
      "Token: nonlinearly  Vector: True  OOV: False\n",
      "Token: separable  Vector: True  OOV: False\n",
      "Token: patterns  Vector: True  OOV: False\n",
      "Token: by  Vector: True  OOV: False\n",
      "Token: proceeding  Vector: True  OOV: False\n",
      "Token: in  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: hybrid  Vector: True  OOV: False\n",
      "Token: manner  Vector: True  OOV: False\n",
      "Token: involving  Vector: True  OOV: False\n",
      "Token: two  Vector: True  OOV: False\n",
      "Token: stages  Vector: True  OOV: False\n",
      "Token: The  Vector: True  OOV: False\n",
      "Token: first  Vector: True  OOV: False\n",
      "Token: stage  Vector: True  OOV: False\n",
      "Token: transforms  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: given  Vector: True  OOV: False\n",
      "Token: set  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: nonlinearly  Vector: True  OOV: False\n",
      "Token: separable  Vector: True  OOV: False\n",
      "Token: patterns  Vector: True  OOV: False\n",
      "Token: into  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: new  Vector: True  OOV: False\n",
      "Token: set  Vector: True  OOV: False\n",
      "Token: for  Vector: True  OOV: False\n",
      "Token: which  Vector: True  OOV: False\n",
      "Token: under  Vector: True  OOV: False\n",
      "Token: certain  Vector: True  OOV: False\n",
      "Token: conditions  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: likelihood  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: transformed  Vector: True  OOV: False\n",
      "Token: patterns  Vector: True  OOV: False\n",
      "Token: becoming  Vector: True  OOV: False\n",
      "Token: linearly  Vector: True  OOV: False\n",
      "Token: separable  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: high  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: mathematical  Vector: True  OOV: False\n",
      "Token: justification  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: this  Vector: True  OOV: False\n",
      "Token: transformation  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: traced  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: an  Vector: True  OOV: False\n",
      "Token: early  Vector: True  OOV: False\n",
      "Token: paper  Vector: True  OOV: False\n",
      "Token: by  Vector: True  OOV: False\n",
      "Token: Cover  Vector: True  OOV: False\n",
      "Token: 1965  Vector: True  OOV: False\n",
      "Token: The  Vector: True  OOV: False\n",
      "Token: second  Vector: True  OOV: False\n",
      "Token: stage  Vector: True  OOV: False\n",
      "Token: completes  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: solution  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: prescribed  Vector: True  OOV: False\n",
      "Token: classification  Vector: True  OOV: False\n",
      "Token: problem  Vector: True  OOV: False\n",
      "Token: by  Vector: True  OOV: False\n",
      "Token: using  Vector: True  OOV: False\n",
      "Token: least  Vector: True  OOV: False\n",
      "Token: squares  Vector: True  OOV: False\n",
      "Token: estimation  Vector: True  OOV: False\n",
      "Token: using  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: radial  Vector: True  OOV: False\n",
      "Token: basis  Vector: True  OOV: False\n",
      "Token: function  Vector: True  OOV: False\n",
      "Token: RBF  Vector: True  OOV: False\n",
      "Token: network  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: structure  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: which  Vector: True  OOV: False\n",
      "Token: consists  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: only  Vector: True  OOV: False\n",
      "Token: three  Vector: True  OOV: False\n",
      "Token: layers  Vector: True  OOV: False\n",
      "Token: The  Vector: True  OOV: False\n",
      "Token: input  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: made  Vector: True  OOV: False\n",
      "Token: up  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: source  Vector: True  OOV: False\n",
      "Token: nodes  Vector: True  OOV: False\n",
      "Token: sensory  Vector: True  OOV: False\n",
      "Token: units  Vector: True  OOV: False\n",
      "Token: that  Vector: True  OOV: False\n",
      "Token: connect  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: network  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: its  Vector: True  OOV: False\n",
      "Token: environment  Vector: True  OOV: False\n",
      "Token: The  Vector: True  OOV: False\n",
      "Token: second  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: consisting  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: hidden  Vector: True  OOV: False\n",
      "Token: units  Vector: True  OOV: False\n",
      "Token: applies  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: nonlinear  Vector: True  OOV: False\n",
      "Token: transformation  Vector: True  OOV: False\n",
      "Token: from  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: input  Vector: True  OOV: False\n",
      "Token: space  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: hidden  Vector: True  OOV: False\n",
      "Token: feature  Vector: True  OOV: False\n",
      "Token: space  Vector: True  OOV: False\n",
      "Token: For  Vector: True  OOV: False\n",
      "Token: most  Vector: True  OOV: False\n",
      "Token: applications  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: dimensionality  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: only  Vector: True  OOV: False\n",
      "Token: hidden  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: network  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: high  Vector: True  OOV: False\n",
      "Token: this  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: trained  Vector: True  OOV: False\n",
      "Token: in  Vector: True  OOV: False\n",
      "Token: an  Vector: True  OOV: False\n",
      "Token: unsupervised  Vector: True  OOV: False\n",
      "Token: manner  Vector: True  OOV: False\n",
      "Token: using  Vector: True  OOV: False\n",
      "Token: stage  Vector: True  OOV: False\n",
      "Token: 1  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: hybrid  Vector: True  OOV: False\n",
      "Token: learning  Vector: True  OOV: False\n",
      "Token: procedure  Vector: True  OOV: False\n",
      "Token: The  Vector: True  OOV: False\n",
      "Token: output  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: linear  Vector: True  OOV: False\n",
      "Token: designed  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: supply  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: response  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: network  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: activation  Vector: True  OOV: False\n",
      "Token: pattern  Vector: True  OOV: False\n",
      "Token: applied  Vector: True  OOV: False\n",
      "Token: to  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: input  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: this  Vector: True  OOV: False\n",
      "Token: layer  Vector: True  OOV: False\n",
      "Token: is  Vector: True  OOV: False\n",
      "Token: trained  Vector: True  OOV: False\n",
      "Token: in  Vector: True  OOV: False\n",
      "Token: a  Vector: True  OOV: False\n",
      "Token: supervised  Vector: True  OOV: False\n",
      "Token: manner  Vector: True  OOV: False\n",
      "Token: using  Vector: True  OOV: False\n",
      "Token: stage  Vector: True  OOV: False\n",
      "Token: 2  Vector: True  OOV: False\n",
      "Token: of  Vector: True  OOV: False\n",
      "Token: the  Vector: True  OOV: False\n",
      "Token: hybrid  Vector: True  OOV: False\n",
      "Token: procedure  Vector: True  OOV: False\n"
     ]
    }
   ],
   "source": [
    "text_spacy = nlp(text_cleaned)\n",
    "for token in text_spacy:\n",
    "    print(\"Token:\", token.text, \" Vector:\", token.has_vector, \" OOV:\", token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a2055-7f9b-4184-8c47-7fcbceab207e",
   "metadata": {},
   "source": [
    "to make a vector we can use the vector method for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f3d51cb-663a-4d0f-9c2d-a663dc0e3162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before stacking\n",
      " [-3.00084829e+00  8.16840112e-01 -8.79860044e-01  1.08452272e+00\n",
      "  5.51078415e+00 -2.28936851e-01  1.36793578e+00  4.38829660e+00\n",
      "  3.62296477e-02 -9.04294789e-01  8.06497574e+00  1.84125233e+00\n",
      " -4.57186174e+00  1.75418544e+00  5.01829505e-01  2.27618361e+00\n",
      "  2.49274421e+00  1.24647856e+00 -1.14556015e+00 -1.95419097e+00\n",
      "  3.10825258e-01 -7.11867630e-01 -1.51449990e+00  1.23233521e+00\n",
      " -4.86318991e-02 -1.71744144e+00 -2.28921771e+00 -1.23251343e+00\n",
      " -9.41623747e-01  2.35453391e+00  1.06821096e+00 -7.23222503e-03\n",
      " -5.49829781e-01 -2.32794929e+00 -2.87862515e+00 -1.06887221e+00\n",
      " -2.10259020e-01  1.44252336e+00  2.02876496e+00  3.66146326e-01\n",
      "  4.30871159e-01 -4.64831084e-01 -1.10738349e+00  7.60156572e-01\n",
      " -2.12335587e+00  2.09006691e+00  2.02235818e+00 -6.42758012e-01\n",
      " -4.60937172e-01  2.85151690e-01 -1.01468766e+00  2.53946424e+00\n",
      " -4.84148651e-01 -4.38228750e+00 -1.33401370e+00  8.78263831e-01\n",
      " -1.84939116e-01  1.54836380e+00  6.36276364e-01 -1.75611186e+00\n",
      "  1.23728669e+00 -8.08898985e-01 -1.01280189e+00 -6.18026853e-01\n",
      "  1.54647136e+00  2.56473684e+00 -2.87099338e+00 -4.38865566e+00\n",
      "  2.12299466e+00  2.77324319e+00 -1.17674839e+00  1.45494807e+00\n",
      " -3.17519689e+00  1.45645484e-01 -7.60425091e-01  1.70315266e+00\n",
      " -2.95817995e+00  2.99957132e+00 -4.88550043e+00 -1.20495595e-01\n",
      " -5.01775789e+00 -2.81892687e-01  8.85334015e-02  1.34214938e+00\n",
      "  2.87187624e+00  2.90360928e-01 -2.56168509e+00 -2.31594944e+00\n",
      "  2.10435748e+00 -4.93417442e-01 -1.66145909e+00  2.31827766e-01\n",
      "  2.68900728e+00 -3.37780881e+00  7.26632655e-01 -1.51659214e+00\n",
      "  2.23916435e+00 -1.81846857e-01 -1.83629822e-02  2.42240739e+00\n",
      "  2.46461344e+00  1.34313750e+00  3.55188084e+00  3.74351501e+00\n",
      " -1.95627466e-01  4.44158506e+00  9.65873420e-01 -2.90905809e+00\n",
      "  6.21252926e-03 -1.99467015e+00  2.00114560e+00  2.47695112e+00\n",
      " -2.70470643e+00  2.39516973e+00  5.69285989e-01 -6.86112404e-01\n",
      " -1.30781567e+00  6.46615922e-01 -2.47309636e-02 -6.46460176e-01\n",
      " -2.36844873e+00 -2.78924680e+00  1.35523641e+00  1.84838068e+00\n",
      " -2.10650945e+00 -4.14481449e+00 -6.89621031e-01 -3.42837071e+00\n",
      "  3.40534925e+00 -1.12549448e+00 -4.15423393e+00  3.28432582e-02\n",
      "  4.28638744e+00  1.74625337e-01 -4.49792385e-01  1.00936091e+00\n",
      " -6.93257272e-01 -1.36986852e+00  2.23771954e+00 -2.05252147e+00\n",
      " -1.34064639e+00  9.44609269e-02  1.91027641e-01  1.05233848e+00\n",
      "  3.08241749e+00  1.21084261e+00 -3.92180514e+00 -3.24760824e-01\n",
      "  1.80103934e+00  3.11615539e+00  2.15055063e-01  3.06641674e+00\n",
      " -3.21254134e-01 -8.83701071e-02 -7.85893202e-01  1.29452872e+00\n",
      "  2.74227786e+00 -4.50562596e-01 -9.24813628e-01 -1.63151348e+00\n",
      " -1.30039930e+00 -8.76017869e-01  1.60483849e+00  1.90857649e+00\n",
      " -1.95817626e+00 -9.75349247e-01 -3.25235248e+00  1.10326517e+00\n",
      "  1.60052800e+00  2.17573896e-01  2.46731266e-01 -1.02583134e+00\n",
      "  2.12991858e+00  9.71339345e-01  1.45191407e+00  9.76004526e-02\n",
      "  5.71851134e-01 -7.77602911e-01 -3.90901971e+00 -1.39062572e+00\n",
      " -1.52682137e+00  3.48467588e-01  9.87166166e-01 -1.37644517e+00\n",
      " -1.71109307e+00 -6.81008339e-01 -2.87748504e+00 -7.84798712e-02\n",
      " -2.18276381e-01  2.11697555e+00  2.44101114e-03 -2.53639913e+00\n",
      "  9.76027131e-01 -1.73009765e+00  1.70417023e+00  2.28404474e+00\n",
      " -3.02627969e+00 -1.16574359e+00  3.89084876e-01 -6.72475338e-01\n",
      " -1.19701695e+00 -2.65387130e+00 -8.01607132e-01 -3.60687637e+00\n",
      "  4.08489513e+00  1.24911261e+00 -5.35063124e+00  1.99832892e+00\n",
      "  9.97581720e-01  4.82147979e-03  2.15591764e+00  2.49500918e+00\n",
      " -2.03281331e+00  1.84913111e+00  1.80880296e+00  2.58934355e+00\n",
      "  1.59616745e+00 -3.13118815e+00 -1.57117486e+00  3.20984304e-01\n",
      " -2.63361335e+00  2.91572523e+00 -1.01549768e+00 -5.28953850e-01\n",
      " -2.09822941e+00 -1.74589002e+00  6.51473880e-01  3.02478600e+00\n",
      "  2.32593727e+00  6.95623532e-02  2.81524420e+00 -3.87503719e+00\n",
      " -3.52380604e-01  2.75873756e+00  1.60851848e+00  1.74473405e+00\n",
      " -9.86575246e-01  3.59284163e-01 -3.26575458e-01  1.26796246e-01\n",
      " -4.05367047e-01 -7.65610695e-01  6.88307136e-02  1.97223306e+00\n",
      " -1.77502900e-01  1.89766693e+00 -3.49523520e+00  6.93223238e-01\n",
      "  1.51096135e-01  1.84228456e+00  2.45101380e+00 -3.20844793e+00\n",
      " -6.03430843e+00 -8.31394017e-01  7.23357499e-01 -2.43085241e+00\n",
      "  3.78175139e+00  8.48368168e-01  3.42402130e-01  1.98371425e-01\n",
      " -7.07164288e-01  6.91312265e+00  3.39521170e+00  3.27287340e+00\n",
      "  1.43660033e+00 -1.66075957e+00 -4.23843786e-02  1.70933473e+00\n",
      " -2.52674079e+00 -3.23738337e-01  8.58946204e-01 -7.49890685e-01\n",
      "  1.18508899e+00 -2.41736174e+00  1.09454048e+00  1.02176942e-01\n",
      "  1.84775424e+00 -6.00514710e-01 -1.23450553e+00  1.48166108e+00\n",
      "  2.18766022e+00 -3.78151909e-02  1.20416343e+00  1.74550867e+00\n",
      "  3.53360581e+00 -1.82631290e+00  2.06075966e-01  1.23137152e+00\n",
      " -8.44910979e-01  3.97265404e-01  2.13184118e-01  1.07350850e+00\n",
      "  2.97789544e-01  1.22759485e+00 -7.43838727e-01  1.78110802e+00\n",
      "  3.27700913e-01 -2.43472600e+00 -1.86512995e+00  9.53451097e-01]\n"
     ]
    }
   ],
   "source": [
    "vectorized_text_spacy = text_spacy.vector\n",
    "print(\"Data before stacking\\n\",vectorized_text_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e6d6e-56d5-4d87-814b-ced75d9af20e",
   "metadata": {},
   "source": [
    "here where you can get the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "50fb524a-b4de-4660-9c1d-af436c8639c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6341067552566528"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"sandwich bread\")\n",
    "doc[0].similarity(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622544b-ee61-4dd7-95d7-5764d494619f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9e440fa-7887-48c3-99a6-f080cbad9e92",
   "metadata": {},
   "source": [
    "# do not forget to do the exercise when your done the course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66399f-ee95-47c2-a0ce-bec41feba181",
   "metadata": {},
   "source": [
    "# All notebook libraries"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85862855-4bb1-42a8-b5d2-2f9988562059",
   "metadata": {},
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp1 = spacy.load(\"en_core_web_lg\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
